{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiband Steering Scan (Configurable Prompt)\n",
    "\n",
    "This notebook performs the complete \"multiband\" scan:\n",
    "- **All 36 layers** (0-35)\n",
    "- **Full α range** [-10, 10] with high resolution\n",
    "- **Configurable prompt** - easily swap topics to test reproducibility\n",
    "\n",
    "**Purpose:** Test whether the signal processing findings (exp(α²) variance growth, KALM boundaries, parabolic log-derivative) are universal properties of the model or prompt-specific artifacts.\n",
    "\n",
    "**Method:** Keep EVERYTHING identical except the prompt text. Compare structure across scans.\n",
    "\n",
    "**Expected runtime:** ~3-5 minutes on H200 (18,000 generations in 72 batches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# EXPERIMENT CONFIGURATION - Edit these to create different scans\n",
    "# ============================================================================\n",
    "\n",
    "# Prompt configuration\n",
    "PROMPT = \"Tell me about the sun. Please do not use Markdown.\"\n",
    "SCAN_NAME = \"sun\"  # Short identifier for output files (no spaces)\n",
    "\n",
    "# Output paths (automatically prefixed with scan name)\n",
    "OUTPUT_CSV = f'../data/results/multiband_scan_{SCAN_NAME}.csv'\n",
    "OUTPUT_JSON = f'../data/results/multiband_scan_{SCAN_NAME}_metadata.json'\n",
    "\n",
    "# ============================================================================\n",
    "# Model and steering configuration\n",
    "# ============================================================================\n",
    "\n",
    "MODEL_NAME = 'Qwen/Qwen3-4B-Instruct-2507'\n",
    "DEVICE = 'cuda'  # H200 SXM\n",
    "VECTOR_PATH = '../data/vectors/complexity_wikipedia.pt'\n",
    "ALL_LAYERS = list(range(36))  # Layers 0-35\n",
    "\n",
    "# ============================================================================\n",
    "# Experiment parameters\n",
    "# ============================================================================\n",
    "\n",
    "ALPHA_MIN = -10.0\n",
    "ALPHA_MAX = 10.0\n",
    "N_ALPHA_SAMPLES = 500  # α values per layer (resolution 0.04)\n",
    "BATCH_SIZE = 1000  # Parallel generations\n",
    "MAX_NEW_TOKENS = 250\n",
    "\n",
    "# Generation parameters\n",
    "TEMPERATURE = 0.0  # Deterministic\n",
    "DO_SAMPLE = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Using device: cuda\n",
      "  GPU: NVIDIA H100 80GB HBM3\n",
      "  VRAM: 84.9 GB\n",
      "\n",
      "Scan configuration:\n",
      "  Name: sun\n",
      "  Prompt: Tell me about the sun. Please do not use Markdown.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from tqdm.auto import tqdm\n",
    "import textstat\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "device = torch.device(DEVICE)\n",
    "print(f\"✓ Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"  GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"  VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "\n",
    "print(f\"\\nScan configuration:\")\n",
    "print(f\"  Name: {SCAN_NAME}\")\n",
    "print(f\"  Prompt: {PROMPT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: Qwen/Qwen3-4B-Instruct-2507...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56707e0ae12a4077aa7e5e2f6b8aed39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Model loaded\n",
      "  Layers: 36\n",
      "  Hidden dim: 2560\n"
     ]
    }
   ],
   "source": [
    "print(f\"Loading model: {MODEL_NAME}...\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=device,\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "print(f\"✓ Model loaded\")\n",
    "print(f\"  Layers: {model.config.num_hidden_layers}\")\n",
    "print(f\"  Hidden dim: {model.config.hidden_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load All Steering Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading steering vectors from ../data/vectors/complexity_wikipedia.pt...\n",
      "✓ Loaded steering vectors for all layers\n",
      "  Shape: torch.Size([36, 2560])\n",
      "  Euclidean norms by layer:\n",
      "    Layer  0: 0.42\n",
      "    Layer 10: 10.77\n",
      "    Layer 20: 14.28\n",
      "    Layer 30: 38.86\n",
      "    Layer 34: 62.46\n",
      "    Layer 35: 24.39\n"
     ]
    }
   ],
   "source": [
    "print(f\"Loading steering vectors from {VECTOR_PATH}...\")\n",
    "vector_data = torch.load(VECTOR_PATH, weights_only=False)\n",
    "\n",
    "# Load all 36 layer vectors\n",
    "steering_vectors = vector_data['vectors'].to(device).to(torch.bfloat16)  # [36, hidden_dim]\n",
    "\n",
    "print(f\"✓ Loaded steering vectors for all layers\")\n",
    "print(f\"  Shape: {steering_vectors.shape}\")\n",
    "print(f\"  Euclidean norms by layer:\")\n",
    "for layer_idx in [0, 10, 20, 30, 34, 35]:\n",
    "    print(f\"    Layer {layer_idx:2d}: {vector_data['layer_norms'][layer_idx]:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Experiment Grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment grid: 36 layers × 500 α values = 18000 total generations\n",
      "  α range: [-10.0, 10.0] with step size 0.0400\n",
      "  Batching: 18 batches of 1000 samples each\n",
      "  Estimated runtime: ~54 seconds (0.9 minutes) at 3s/batch\n"
     ]
    }
   ],
   "source": [
    "# Generate all α values (same for each layer)\n",
    "alphas = np.linspace(ALPHA_MIN, ALPHA_MAX, N_ALPHA_SAMPLES)\n",
    "\n",
    "# Create experiment grid: (layer, alpha) pairs\n",
    "experiment_grid = []\n",
    "for layer in ALL_LAYERS:\n",
    "    for alpha in alphas:\n",
    "        experiment_grid.append((layer, alpha))\n",
    "\n",
    "total_experiments = len(experiment_grid)\n",
    "n_batches = int(np.ceil(total_experiments / BATCH_SIZE))\n",
    "\n",
    "print(f\"Experiment grid: {len(ALL_LAYERS)} layers × {N_ALPHA_SAMPLES} α values = {total_experiments} total generations\")\n",
    "print(f\"  α range: [{ALPHA_MIN}, {ALPHA_MAX}] with step size {(ALPHA_MAX - ALPHA_MIN) / N_ALPHA_SAMPLES:.4f}\")\n",
    "print(f\"  Batching: {n_batches} batches of {BATCH_SIZE} samples each\")\n",
    "print(f\"  Estimated runtime: ~{n_batches * 3:.0f} seconds ({n_batches * 3 / 60:.1f} minutes) at 3s/batch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batched Generation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing batched generation with (layer, α) = [(0, 0.0), (34, 1.0)]...\n",
      "✓ Generated 2 completions\n",
      "  Sample 1 (L0, α=0): 197 words\n",
      "  Sample 2 (L34, α=1): 203 words\n"
     ]
    }
   ],
   "source": [
    "def generate_batch_with_layer_steering(prompt, layer_alpha_pairs):\n",
    "    \"\"\"\n",
    "    Generate multiple completions with different (layer, α) pairs in a single batch.\n",
    "    \n",
    "    Args:\n",
    "        prompt: Input text (will be formatted as user message)\n",
    "        layer_alpha_pairs: List of (layer_idx, alpha) tuples\n",
    "    \n",
    "    Returns:\n",
    "        List of completions (one per pair)\n",
    "    \"\"\"\n",
    "    batch_size = len(layer_alpha_pairs)\n",
    "    \n",
    "    # Format prompt using chat template\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    formatted_prompt = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    \n",
    "    # Tokenize and replicate for batch\n",
    "    inputs = tokenizer([formatted_prompt] * batch_size, return_tensors='pt', padding=True).to(device)\n",
    "    \n",
    "    # Organize by layer (hook registration is per-layer)\n",
    "    layer_to_samples = {}\n",
    "    for batch_idx, (layer, alpha) in enumerate(layer_alpha_pairs):\n",
    "        if layer not in layer_to_samples:\n",
    "            layer_to_samples[layer] = []\n",
    "        layer_to_samples[layer].append((batch_idx, alpha))\n",
    "    \n",
    "    # Register hooks for all active layers\n",
    "    hook_handles = []\n",
    "    \n",
    "    for layer, samples in layer_to_samples.items():\n",
    "        # Precompute which batch indices get steered at this layer\n",
    "        batch_indices = torch.tensor([s[0] for s in samples], device=device, dtype=torch.long)\n",
    "        batch_alphas = torch.tensor([s[1] for s in samples], device=device, dtype=torch.bfloat16)\n",
    "        steering_vector = steering_vectors[layer]  # [hidden_dim]\n",
    "        \n",
    "        def make_steering_hook(batch_indices, batch_alphas, steering_vector):\n",
    "            def steering_hook(module, input, output):\n",
    "                # Extract hidden states\n",
    "                if isinstance(output, tuple):\n",
    "                    hidden_states = output[0]\n",
    "                else:\n",
    "                    hidden_states = output\n",
    "                \n",
    "                # Apply per-sample steering ONLY to samples targeting this layer\n",
    "                for i, alpha in zip(batch_indices, batch_alphas):\n",
    "                    hidden_states[i] = hidden_states[i] + alpha * steering_vector\n",
    "                \n",
    "                if isinstance(output, tuple):\n",
    "                    return (hidden_states,) + output[1:]\n",
    "                else:\n",
    "                    return hidden_states\n",
    "            return steering_hook\n",
    "        \n",
    "        # Register hook for this layer\n",
    "        target_layer = model.model.layers[layer]\n",
    "        hook = make_steering_hook(batch_indices, batch_alphas, steering_vector)\n",
    "        hook_handle = target_layer.register_forward_hook(hook)\n",
    "        hook_handles.append(hook_handle)\n",
    "    \n",
    "    try:\n",
    "        # Generate\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=MAX_NEW_TOKENS,\n",
    "                temperature=TEMPERATURE if TEMPERATURE > 0 else None,\n",
    "                do_sample=DO_SAMPLE,\n",
    "                pad_token_id=tokenizer.pad_token_id or tokenizer.eos_token_id,\n",
    "            )\n",
    "        \n",
    "        # Decode all outputs\n",
    "        completions = []\n",
    "        for output in outputs:\n",
    "            full_text = tokenizer.decode(output, skip_special_tokens=True)\n",
    "            # Extract just assistant response\n",
    "            completion = full_text.split(\"assistant\\n\")[-1].strip()\n",
    "            completions.append(completion)\n",
    "        \n",
    "        return completions\n",
    "    \n",
    "    finally:\n",
    "        # Always remove all hooks\n",
    "        for handle in hook_handles:\n",
    "            handle.remove()\n",
    "\n",
    "# Test with small batch\n",
    "print(\"Testing batched generation with (layer, α) = [(0, 0.0), (34, 1.0)]...\")\n",
    "test_completions = generate_batch_with_layer_steering(PROMPT, [(0, 0.0), (34, 1.0)])\n",
    "print(f\"✓ Generated {len(test_completions)} completions\")\n",
    "print(f\"  Sample 1 (L0, α=0): {len(test_completions[0].split())} words\")\n",
    "print(f\"  Sample 2 (L34, α=1): {len(test_completions[1].split())} words\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Full Multiband Scan 🚀"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running multiband scan 'sun': 18000 generations across 18 batches\n",
      "\n",
      "Prompt: Tell me about the sun. Please do not use Markdown.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a136f0e3e7d41079b3241ecbdebf885",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing batches:   0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Completed 18000 generations across 36 layers\n",
      "\n",
      "Grade level range: -3.4 to 383.4\n",
      "Reading ease range: -2632.4 to 121.2\n",
      "Mean words per generation: 183.0\n"
     ]
    }
   ],
   "source": [
    "print(f\"Running multiband scan '{SCAN_NAME}': {total_experiments} generations across {n_batches} batches\\n\")\n",
    "print(f\"Prompt: {PROMPT}\\n\")\n",
    "\n",
    "results = []\n",
    "\n",
    "# Split experiment grid into batches\n",
    "for batch_start in tqdm(range(0, total_experiments, BATCH_SIZE), desc=\"Processing batches\"):\n",
    "    batch_end = min(batch_start + BATCH_SIZE, total_experiments)\n",
    "    batch_pairs = experiment_grid[batch_start:batch_end]\n",
    "    \n",
    "    # Generate entire batch\n",
    "    completions = generate_batch_with_layer_steering(PROMPT, batch_pairs)\n",
    "    \n",
    "    # Compute metrics for each completion\n",
    "    for (layer, alpha), completion in zip(batch_pairs, completions):\n",
    "        # Compute grade level and reading ease\n",
    "        try:\n",
    "            grade_level = textstat.flesch_kincaid_grade(completion)\n",
    "            reading_ease = textstat.flesch_reading_ease(completion)\n",
    "        except:\n",
    "            grade_level = np.nan\n",
    "            reading_ease = np.nan\n",
    "        \n",
    "        # Count words and sentences\n",
    "        words = completion.split()\n",
    "        n_words = len(words)\n",
    "        n_sentences = completion.count('.') + completion.count('!') + completion.count('?')\n",
    "        \n",
    "        # Compute diagnostics\n",
    "        avg_word_length = np.mean([len(w) for w in words]) if words else 0\n",
    "        avg_sentence_length = n_words / n_sentences if n_sentences > 0 else 0\n",
    "        \n",
    "        # Store\n",
    "        results.append({\n",
    "            'layer': layer,\n",
    "            'alpha': float(alpha),\n",
    "            'grade_level': grade_level,\n",
    "            'reading_ease': reading_ease,\n",
    "            'n_words': n_words,\n",
    "            'n_sentences': n_sentences,\n",
    "            'avg_word_length': avg_word_length,\n",
    "            'avg_sentence_length': avg_sentence_length,\n",
    "            'completion': completion,\n",
    "        })\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(results)\n",
    "\n",
    "print(f\"\\n✓ Completed {len(df)} generations across {len(ALL_LAYERS)} layers\")\n",
    "print(f\"\\nGrade level range: {df['grade_level'].min():.1f} to {df['grade_level'].max():.1f}\")\n",
    "print(f\"Reading ease range: {df['reading_ease'].min():.1f} to {df['reading_ease'].max():.1f}\")\n",
    "print(f\"Mean words per generation: {df['n_words'].mean():.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick Diagnostics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 5 layers by coherent fraction:\n",
      " layer  coherent_fraction  alpha_min_coherent  alpha_max_coherent\n",
      "     0                1.0               -10.0                10.0\n",
      "     2                1.0               -10.0                10.0\n",
      "     3                1.0               -10.0                10.0\n",
      "     4                1.0               -10.0                10.0\n",
      "     5                1.0               -10.0                10.0\n",
      "\n",
      "Bottom 5 layers by coherent fraction:\n",
      " layer  coherent_fraction  alpha_min_coherent  alpha_max_coherent\n",
      "    34              0.682          -10.000000            5.951904\n",
      "    33              0.696          -10.000000            6.833667\n",
      "    25              0.702           -6.633267            8.316633\n",
      "    30              0.702           -6.593186            8.116232\n",
      "    28              0.704           -6.793587            7.875752\n"
     ]
    }
   ],
   "source": [
    "# Define \"coherent\" as: grade level < 50 and n_sentences > 0\n",
    "df['coherent'] = (df['grade_level'] < 50) & (df['n_sentences'] > 0)\n",
    "\n",
    "# Quick layer summary\n",
    "layer_stats = []\n",
    "for layer in ALL_LAYERS:\n",
    "    layer_df = df[df['layer'] == layer]\n",
    "    coherent_samples = layer_df[layer_df['coherent']]\n",
    "    \n",
    "    if len(coherent_samples) > 0:\n",
    "        alpha_min = coherent_samples['alpha'].min()\n",
    "        alpha_max = coherent_samples['alpha'].max()\n",
    "        coherent_fraction = len(coherent_samples) / len(layer_df)\n",
    "        mean_grade = coherent_samples['grade_level'].mean()\n",
    "    else:\n",
    "        alpha_min = np.nan\n",
    "        alpha_max = np.nan\n",
    "        coherent_fraction = 0.0\n",
    "        mean_grade = np.nan\n",
    "    \n",
    "    layer_stats.append({\n",
    "        'layer': layer,\n",
    "        'alpha_min_coherent': alpha_min,\n",
    "        'alpha_max_coherent': alpha_max,\n",
    "        'coherent_fraction': coherent_fraction,\n",
    "        'mean_grade_level': mean_grade,\n",
    "    })\n",
    "\n",
    "layer_stats_df = pd.DataFrame(layer_stats)\n",
    "\n",
    "print(\"\\nTop 5 layers by coherent fraction:\")\n",
    "print(layer_stats_df.nlargest(5, 'coherent_fraction')[['layer', 'coherent_fraction', 'alpha_min_coherent', 'alpha_max_coherent']].to_string(index=False))\n",
    "\n",
    "print(\"\\nBottom 5 layers by coherent fraction:\")\n",
    "print(layer_stats_df.nsmallest(5, 'coherent_fraction')[['layer', 'coherent_fraction', 'alpha_min_coherent', 'alpha_max_coherent']].to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Output (α=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Layer 0 | α = -0.020 | Grade Level = 10.4\n",
      "Coherent = True | Words = 178\n",
      "================================================================================\n",
      "The Sun is a star located at the center of our solar system. It is a massive ball of hot, glowing gas, primarily composed of hydrogen and helium. The Sun's immense gravity holds the planets, moons, asteroids, and comets in orbit around it. It generates energy through a process called nuclear fusion,\n",
      "...\n",
      "\n",
      "================================================================================\n",
      "Layer 15 | α = -0.020 | Grade Level = 10.0\n",
      "Coherent = True | Words = 180\n",
      "================================================================================\n",
      "The Sun is a star located at the center of our solar system. It is a massive ball of hot, glowing gas, primarily composed of hydrogen and helium. The Sun's immense gravity holds together the planets, moons, asteroids, and comets that orbit around it. It generates energy through a process called nucl\n",
      "...\n",
      "\n",
      "================================================================================\n",
      "Layer 34 | α = -0.020 | Grade Level = 9.7\n",
      "Coherent = True | Words = 198\n",
      "================================================================================\n",
      "The Sun is a star located at the center of our solar system. It is a massive ball of hot, glowing gas, primarily composed of hydrogen and helium. The Sun's immense gravity holds the planets, moons, asteroids, and comets in orbit around it. It generates energy through a process called nuclear fusion,\n",
      "...\n",
      "\n",
      "================================================================================\n",
      "Layer 35 | α = -0.020 | Grade Level = 9.9\n",
      "Coherent = True | Words = 151\n",
      "================================================================================\n",
      "The Sun is a star located at the center of our solar system. It is a massive ball of hot, glowing gas, primarily composed of hydrogen and helium. The Sun's immense gravity holds the planets, moons, asteroids, and comets in orbit around it. It generates energy through a process called nuclear fusion,\n",
      "...\n"
     ]
    }
   ],
   "source": [
    "# Show output at α≈0 for a few representative layers\n",
    "sample_layers = [0, 15, 34, 35]\n",
    "\n",
    "for layer in sample_layers:\n",
    "    layer_df = df[df['layer'] == layer]\n",
    "    \n",
    "    # Find α≈0 sample\n",
    "    idx = (layer_df['alpha'] - 0.0).abs().idxmin()\n",
    "    row = layer_df.loc[idx]\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Layer {int(row['layer'])} | α = {row['alpha']:.3f} | Grade Level = {row['grade_level']:.1f}\")\n",
    "    print(f\"Coherent = {row['coherent']} | Words = {row['n_words']}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(row['completion'][:300])\n",
    "    if len(row['completion']) > 300:\n",
    "        print(\"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Saved results to ../data/results/multiband_scan_sun.csv\n",
      "✓ Saved metadata to ../data/results/multiband_scan_sun_metadata.json\n",
      "\n",
      "Total output size: 19.62 MB\n"
     ]
    }
   ],
   "source": [
    "# Save DataFrame\n",
    "df.to_csv(OUTPUT_CSV, index=False)\n",
    "print(f\"✓ Saved results to {OUTPUT_CSV}\")\n",
    "\n",
    "# Save metadata\n",
    "metadata = {\n",
    "    'scan_name': SCAN_NAME,\n",
    "    'prompt': PROMPT,\n",
    "    'model': MODEL_NAME,\n",
    "    'vector_path': VECTOR_PATH,\n",
    "    'layers': ALL_LAYERS,\n",
    "    'alpha_range': [float(ALPHA_MIN), float(ALPHA_MAX)],\n",
    "    'n_alpha_samples': N_ALPHA_SAMPLES,\n",
    "    'total_generations': total_experiments,\n",
    "    'batch_size': BATCH_SIZE,\n",
    "    'max_new_tokens': MAX_NEW_TOKENS,\n",
    "    'temperature': TEMPERATURE,\n",
    "    'do_sample': DO_SAMPLE,\n",
    "    'layer_statistics': layer_stats_df.to_dict(orient='records'),\n",
    "    'timestamp': datetime.now().isoformat(),\n",
    "}\n",
    "\n",
    "with open(OUTPUT_JSON, 'w') as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "print(f\"✓ Saved metadata to {OUTPUT_JSON}\")\n",
    "\n",
    "file_size_mb = sum([\n",
    "    os.path.getsize(OUTPUT_CSV),\n",
    "    os.path.getsize(OUTPUT_JSON)\n",
    "]) / (1024 * 1024)\n",
    "print(f\"\\nTotal output size: {file_size_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Multiband scan complete! 🚀\n",
    "\n",
    "**What we captured:**\n",
    "- 36 layers × 500 α values = 18,000 generations\n",
    "- Flesch-Kincaid grade level + reading ease\n",
    "- Word/sentence counts and diagnostics\n",
    "- Full completion text for qualitative analysis\n",
    "\n",
    "**Next steps:**\n",
    "1. Download data to laptop\n",
    "2. Run signal processing analysis (consensus signal, variance, derivative)\n",
    "3. Compare structure across different prompts (QM vs sun vs others)\n",
    "4. Test universality of exp(α²) signature"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
