{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec Test: Euclidean Geometry\n",
    "\n",
    "**Goal:** Test the canonical word2vec arithmetic in **Euclidean space**.\n",
    "\n",
    "**Hypothesis:** `'woman' - 'man' + 'king' ≈ 'queen'`\n",
    "\n",
    "**Why this matters:** This is the **classic test** of linear semantic relationships. If it works in Euclidean space, we know the raw embeddings have compositional structure. We'll compare with 07.59b (causal metric) to see if the metric preserves or destroys this structure.\n",
    "\n",
    "**Method:**\n",
    "1. Compute synthetic vector: `v = 'woman' - 'man' + 'king'`\n",
    "2. Find nearest neighbors by **Euclidean distance**\n",
    "3. Find nearest neighbors by **Euclidean cosine similarity**\n",
    "4. Check where 'queen' ranks\n",
    "\n",
    "**Note:** We search the **full vocabulary** (151k tokens), not just our 32k sample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration:\n",
      "  Model: Qwen/Qwen3-4B-Instruct-2507\n",
      "  Arithmetic: 'woman' - 'man' + 'king' = ?\n",
      "  Metric: EUCLIDEAN\n",
      "  Search space: Full vocabulary (151k tokens)\n",
      "  Top N results: 20\n"
     ]
    }
   ],
   "source": [
    "# Model\n",
    "MODEL_NAME = 'Qwen/Qwen3-4B-Instruct-2507'\n",
    "\n",
    "# Tokens for arithmetic\n",
    "TOKENS = {\n",
    "    'man': None,    # Will tokenize\n",
    "    'woman': None,  # Will tokenize\n",
    "    'king': None,   # Will tokenize\n",
    "    'queen': None,  # Will tokenize (for checking)\n",
    "}\n",
    "\n",
    "# Analysis parameters\n",
    "TOP_N = 20  # How many neighbors to show\n",
    "\n",
    "print(f\"Configuration:\")\n",
    "print(f\"  Model: {MODEL_NAME}\")\n",
    "print(f\"  Arithmetic: 'woman' - 'man' + 'king' = ?\")\n",
    "print(f\"  Metric: EUCLIDEAN\")\n",
    "print(f\"  Search space: Full vocabulary (151k tokens)\")\n",
    "print(f\"  Top N results: {TOP_N}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Imports complete\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "print(\"✓ Imports complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model and tokenizer...\n",
      "\n",
      "Loading tokenizer from Qwen/Qwen3-4B-Instruct-2507...\n",
      "\n",
      "Loading model (for unembedding matrix)...\n",
      "  This will take a minute...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9aeb65fc609a4697abe46cb7bb56278b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Model loaded\n",
      "  Vocab size: 151,643\n",
      "  Unembedding matrix shape: torch.Size([151936, 2560])\n",
      "  Memory: 1.56 GB\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading model and tokenizer...\\n\")\n",
    "\n",
    "# Tokenizer\n",
    "print(f\"Loading tokenizer from {MODEL_NAME}...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Model (for full unembedding matrix)\n",
    "print(f\"\\nLoading model (for unembedding matrix)...\")\n",
    "print(\"  This will take a minute...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map='cpu',\n",
    ")\n",
    "\n",
    "# Extract FULL unembedding matrix (all vocab)\n",
    "gamma = model.lm_head.weight.data.to(torch.float32).cpu()  # [vocab_size, hidden_dim]\n",
    "\n",
    "print(f\"\\n✓ Model loaded\")\n",
    "print(f\"  Vocab size: {tokenizer.vocab_size:,}\")\n",
    "print(f\"  Unembedding matrix shape: {gamma.shape}\")\n",
    "print(f\"  Memory: {gamma.element_size() * gamma.nelement() / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize and Verify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing target words...\n",
      "\n",
      "✓ 'man' → token 1515: 'man'\n",
      "✓ 'woman' → token 22028: 'woman'\n",
      "✓ 'king' → token 10566: 'king'\n",
      "✓ 'queen' → token 93114: 'queen'\n",
      "\n",
      "✓ All words are single tokens\n"
     ]
    }
   ],
   "source": [
    "print(\"Tokenizing target words...\\n\")\n",
    "\n",
    "words = ['man', 'woman', 'king', 'queen']\n",
    "all_single_tokens = True\n",
    "\n",
    "for word in words:\n",
    "    tokens = tokenizer.encode(word, add_special_tokens=False)\n",
    "    \n",
    "    if len(tokens) == 1:\n",
    "        token_id = tokens[0]\n",
    "        TOKENS[word] = token_id\n",
    "        text = tokenizer.decode([token_id])\n",
    "        print(f\"✓ '{word}' → token {token_id}: '{text}'\")\n",
    "    else:\n",
    "        print(f\"✗ '{word}' → {len(tokens)} tokens: {tokens}\")\n",
    "        all_single_tokens = False\n",
    "\n",
    "if not all_single_tokens:\n",
    "    print(\"\\n⚠️  Not all words are single tokens!\")\n",
    "    print(\"    This may affect results, but we'll proceed anyway.\")\n",
    "else:\n",
    "    print(\"\\n✓ All words are single tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Synthetic Vector: 'woman' - 'man' + 'king'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Computing synthetic vector...\n",
      "\n",
      "v_synthetic = 'woman' - 'man' + 'king'\n",
      "\n",
      "Synthetic vector properties:\n",
      "  Shape: torch.Size([2560])\n",
      "  Euclidean norm: 1.81\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nComputing synthetic vector...\\n\")\n",
    "\n",
    "# Get embeddings\n",
    "v_man = gamma[TOKENS['man']]\n",
    "v_woman = gamma[TOKENS['woman']]\n",
    "v_king = gamma[TOKENS['king']]\n",
    "\n",
    "# Arithmetic\n",
    "v_synthetic = v_woman - v_man + v_king\n",
    "\n",
    "print(f\"v_synthetic = 'woman' - 'man' + 'king'\")\n",
    "print(f\"\\nSynthetic vector properties:\")\n",
    "print(f\"  Shape: {v_synthetic.shape}\")\n",
    "print(f\"  Euclidean norm: {torch.norm(v_synthetic).item():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Direct Vector Comparison: Are the Transformations Parallel?\n",
    "\n",
    "**Simple test:** Do `woman - man` and `queen - king` point in the same direction?\n",
    "\n",
    "If word2vec arithmetic works, these should be **parallel** (or at least highly aligned)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "DIRECT VECTOR COMPARISON (EUCLIDEAN)\n",
      "================================================================================\n",
      "\n",
      "(woman - man) properties:\n",
      "  Euclidean norm: 1.3019\n",
      "\n",
      "(queen - king) properties:\n",
      "  Euclidean norm: 1.4175\n",
      "\n",
      "Alignment between the two vectors:\n",
      "  Dot product: 0.2379\n",
      "  Cosine similarity: 0.1289\n",
      "  Angle: 82.59°\n",
      "\n",
      "================================================================================\n",
      "INTERPRETATION:\n",
      "================================================================================\n",
      "❌ Vectors are nearly ORTHOGONAL (cos=0.129, angle=82.6°)\n",
      "   No meaningful linear relationship\n",
      "   Word2vec arithmetic will NOT work\n",
      "\n",
      "For reference:\n",
      "  • Parallel vectors: cos=1.0, angle=0°\n",
      "  • Orthogonal vectors: cos=0.0, angle=90°\n",
      "  • Opposite vectors: cos=-1.0, angle=180°\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"DIRECT VECTOR COMPARISON (EUCLIDEAN)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Get queen vector\n",
    "v_queen = gamma[TOKENS['queen']]\n",
    "\n",
    "# Compute the two displacement vectors\n",
    "v_gender_shift = v_woman - v_man      # man → woman\n",
    "v_royalty_shift = v_queen - v_king    # king → queen\n",
    "\n",
    "# Euclidean properties\n",
    "norm_gender = torch.norm(v_gender_shift).item()\n",
    "norm_royalty = torch.norm(v_royalty_shift).item()\n",
    "\n",
    "# Cosine similarity and angle\n",
    "dot_product = torch.dot(v_gender_shift, v_royalty_shift).item()\n",
    "cosine_similarity = dot_product / (norm_gender * norm_royalty)\n",
    "angle_rad = torch.acos(torch.clamp(torch.tensor(cosine_similarity), -1.0, 1.0))\n",
    "angle_deg = torch.rad2deg(angle_rad).item()\n",
    "\n",
    "print(f\"\\n(woman - man) properties:\")\n",
    "print(f\"  Euclidean norm: {norm_gender:.4f}\")\n",
    "\n",
    "print(f\"\\n(queen - king) properties:\")\n",
    "print(f\"  Euclidean norm: {norm_royalty:.4f}\")\n",
    "\n",
    "print(f\"\\nAlignment between the two vectors:\")\n",
    "print(f\"  Dot product: {dot_product:.4f}\")\n",
    "print(f\"  Cosine similarity: {cosine_similarity:.4f}\")\n",
    "print(f\"  Angle: {angle_deg:.2f}°\")\n",
    "\n",
    "print(f\"\\n\" + \"=\" * 80)\n",
    "print(\"INTERPRETATION:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if abs(cosine_similarity) > 0.9:\n",
    "    print(f\"✅ Vectors are HIGHLY ALIGNED (cos={cosine_similarity:.3f})\")\n",
    "    print(f\"   Word2vec arithmetic should work!\")\n",
    "elif abs(cosine_similarity) > 0.7:\n",
    "    print(f\"✓ Vectors are moderately aligned (cos={cosine_similarity:.3f})\")\n",
    "    print(f\"  Some linear structure exists\")\n",
    "elif abs(cosine_similarity) > 0.3:\n",
    "    print(f\"⚠️  Vectors are weakly aligned (cos={cosine_similarity:.3f})\")\n",
    "    print(f\"   Minimal linear relationship\")\n",
    "else:\n",
    "    print(f\"❌ Vectors are nearly ORTHOGONAL (cos={cosine_similarity:.3f}, angle={angle_deg:.1f}°)\")\n",
    "    print(f\"   No meaningful linear relationship\")\n",
    "    print(f\"   Word2vec arithmetic will NOT work\")\n",
    "\n",
    "print(f\"\\nFor reference:\")\n",
    "print(f\"  • Parallel vectors: cos=1.0, angle=0°\")\n",
    "print(f\"  • Orthogonal vectors: cos=0.0, angle=90°\")\n",
    "print(f\"  • Opposite vectors: cos=-1.0, angle=180°\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find Nearest Neighbors by Euclidean Distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Scaled Vector Arithmetic\n",
    "\n",
    "**Question:** Is the failure due to **magnitude** or **direction**?\n",
    "\n",
    "**Hypothesis:** Maybe `queen = king + α * (woman - man)` for some optimal α ≠ 1\n",
    "\n",
    "**Method:** Solve for optimal α via least-squares projection:\n",
    "\n",
    "```\n",
    "α = (queen - king) · (woman - man) / ||woman - man||²\n",
    "```\n",
    "\n",
    "If α ≈ 0, the vectors are **orthogonal** (no relationship).\n",
    "\n",
    "If α is reasonable but residual is large, the direction is right but imperfect.\n",
    "\n",
    "If α ≈ 1 and residual is small, we just needed scaling!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "SCALED VECTOR ARITHMETIC TEST (EUCLIDEAN)\n",
      "================================================================================\n",
      "\n",
      "Optimal scaling factor α = 0.140366\n",
      "\n",
      "Interpretation:\n",
      "  α = 0.14 → Vectors are aligned but need rescaling\n",
      "  Partial compositional structure\n",
      "\n",
      "Method                         Distance        Angle (degrees)\n",
      "============================================================\n",
      "Original (α=1.0)               1.7968          70.84          \n",
      "Scaled (α=0.1404)              1.4057          71.69          \n",
      "\n",
      "Distance improvement: 21.8%\n",
      "Angular improvement: -1.2%\n",
      "  → Moderate distance improvement. Direction is partially correct\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"SCALED VECTOR ARITHMETIC TEST (EUCLIDEAN)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Define vectors\n",
    "v_gender_shift = v_woman - v_man  # The gender transformation vector\n",
    "v_queen = gamma[TOKENS['queen']]\n",
    "v_target_displacement = v_queen - v_king  # What we actually need to add to king\n",
    "\n",
    "# Solve for optimal α: queen = king + α * (woman - man)\n",
    "# α = (queen - king) · (woman - man) / ||woman - man||²\n",
    "numerator = torch.dot(v_target_displacement, v_gender_shift).item()\n",
    "denominator = torch.dot(v_gender_shift, v_gender_shift).item()\n",
    "alpha_optimal = numerator / denominator\n",
    "\n",
    "print(f\"\\nOptimal scaling factor α = {alpha_optimal:.6f}\")\n",
    "print(f\"\\nInterpretation:\")\n",
    "if abs(alpha_optimal) < 0.01:\n",
    "    print(f\"  α ≈ 0 → Gender shift is ORTHOGONAL to king→queen displacement\")\n",
    "    print(f\"  The vectors have no meaningful relationship\")\n",
    "elif 0.8 <= alpha_optimal <= 1.2:\n",
    "    print(f\"  α ≈ 1 → Gender shift aligns well with king→queen displacement!\")\n",
    "    print(f\"  Word2vec arithmetic works (just needed right magnitude)\")\n",
    "else:\n",
    "    print(f\"  α = {alpha_optimal:.2f} → Vectors are aligned but need rescaling\")\n",
    "    print(f\"  Partial compositional structure\")\n",
    "\n",
    "# Construct scaled synthetic vector\n",
    "v_scaled = v_king + alpha_optimal * v_gender_shift\n",
    "\n",
    "# === EUCLIDEAN DISTANCE ===\n",
    "euclidean_dist_synthetic_to_queen = torch.norm(v_synthetic - v_queen).item()\n",
    "euclidean_dist_scaled_to_queen = torch.norm(v_scaled - v_queen).item()\n",
    "\n",
    "# === EUCLIDEAN ANGLE ===\n",
    "# Compute cosine and convert to degrees\n",
    "cos_synthetic_queen = torch.dot(v_synthetic, v_queen) / (torch.norm(v_synthetic) * torch.norm(v_queen))\n",
    "angle_synthetic_queen_rad = torch.acos(torch.clamp(cos_synthetic_queen, -1.0, 1.0))\n",
    "angle_synthetic_queen_deg = torch.rad2deg(angle_synthetic_queen_rad).item()\n",
    "\n",
    "cos_scaled_queen = torch.dot(v_scaled, v_queen) / (torch.norm(v_scaled) * torch.norm(v_queen))\n",
    "angle_scaled_queen_rad = torch.acos(torch.clamp(cos_scaled_queen, -1.0, 1.0))\n",
    "angle_scaled_queen_deg = torch.rad2deg(angle_scaled_queen_rad).item()\n",
    "\n",
    "print(f\"\\n{'Method':<30} {'Distance':<15} {'Angle (degrees)':<15}\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"{'Original (α=1.0)':<30} {euclidean_dist_synthetic_to_queen:<15.4f} {angle_synthetic_queen_deg:<15.2f}\")\n",
    "print(f\"{'Scaled (α=' + f'{alpha_optimal:.4f})':<30} {euclidean_dist_scaled_to_queen:<15.4f} {angle_scaled_queen_deg:<15.2f}\")\n",
    "\n",
    "dist_improvement = (1 - euclidean_dist_scaled_to_queen / euclidean_dist_synthetic_to_queen) * 100\n",
    "angle_improvement = (1 - angle_scaled_queen_deg / angle_synthetic_queen_deg) * 100\n",
    "\n",
    "print(f\"\\nDistance improvement: {dist_improvement:.1f}%\")\n",
    "print(f\"Angular improvement: {angle_improvement:.1f}%\")\n",
    "\n",
    "if dist_improvement > 50:\n",
    "    print(\"  → LARGE distance improvement! The issue was magnitude, not direction\")\n",
    "elif dist_improvement > 10:\n",
    "    print(\"  → Moderate distance improvement. Direction is partially correct\")\n",
    "else:\n",
    "    print(\"  → Minimal distance improvement. The vectors are not well aligned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Computing Euclidean distances to ALL tokens in vocabulary...\n",
      "\n",
      "  (This may take a minute for 151k tokens...)\n",
      "\n",
      "Top 20 tokens by EUCLIDEAN DISTANCE:\n",
      "Rank   Distance     Token ID   Text                                    \n",
      "================================================================================\n",
      "1      1.30         10566      king                                    \n",
      "2      1.60         22028      woman                                   \n",
      "3      1.69         33555      King                                    \n",
      "4      1.71         11477       king                                   \n",
      "5      1.72         73811       KING                                   \n",
      "6      1.73         64662      women                                   \n",
      "7      1.74         148785     𝙠                                       \n",
      "8      1.74         151607     ﱊ                                       \n",
      "9      1.74         150876     끅                                       \n",
      "10     1.74         149446     ﳈ                                       \n",
      "11     1.74         151273     𒄷                                       \n",
      "12     1.74         151489     컁                                       \n",
      "13     1.74         148550     껭                                       \n",
      "14     1.74         151288     𝄱                                       \n",
      "15     1.74         151110     혭                                       \n",
      "16     1.74         148659     퀫                                       \n",
      "17     1.74         151099     핶                                       \n",
      "18     1.74         123302     𡐓                                       \n",
      "19     1.74         148886     ﱉ                                       \n",
      "20     1.74         151548     켇                                       \n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nComputing Euclidean distances to ALL tokens in vocabulary...\\n\")\n",
    "print(\"  (This may take a minute for 151k tokens...)\\n\")\n",
    "\n",
    "# Compute Euclidean distances: ||u - v||_2\n",
    "vocab_size = gamma.shape[0]\n",
    "euclidean_distances = torch.norm(gamma - v_synthetic, dim=1).numpy()\n",
    "\n",
    "# Sort by distance (ascending)\n",
    "sorted_indices = np.argsort(euclidean_distances)\n",
    "top_indices = sorted_indices[:TOP_N]\n",
    "\n",
    "print(f\"Top {TOP_N} tokens by EUCLIDEAN DISTANCE:\")\n",
    "print(f\"{'Rank':<6} {'Distance':<12} {'Token ID':<10} {'Text':<40}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for rank, idx in enumerate(top_indices, 1):\n",
    "    dist = euclidean_distances[idx]\n",
    "    text = tokenizer.decode([idx])\n",
    "    print(f\"{rank:<6} {dist:<12.2f} {idx:<10} {text:<40}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find Nearest Neighbors by Euclidean Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Computing Euclidean cosine similarities to ALL tokens...\n",
      "\n",
      "Top 20 tokens by EUCLIDEAN COSINE SIMILARITY:\n",
      "Rank   Cosine       Distance     Token ID   Text                                    \n",
      "================================================================================\n",
      "1      0.6945       1.30         10566      king                                    \n",
      "2      0.4840       1.60         22028      woman                                   \n",
      "3      0.4002       1.69         33555      King                                    \n",
      "4      0.3839       1.72         73811       KING                                   \n",
      "5      0.3806       1.71         11477       king                                   \n",
      "6      0.3771       1.73         64662      women                                   \n",
      "7      0.3472       1.75         27906       queen                                  \n",
      "8      0.3332       1.79         95049      Woman                                   \n",
      "9      0.3283       1.80         93114      queen                                   \n",
      "10     0.3276       1.77         6210        King                                   \n",
      "11     0.3232       1.77         35090      Women                                   \n",
      "12     0.3164       1.78         25079       kingdom                                \n",
      "13     0.3157       1.79         44519       kings                                  \n",
      "14     0.3120       1.76         5220        woman                                  \n",
      "15     0.3077       1.82         107894     国王                                      \n",
      "16     0.2982       1.85         101989     女人                                      \n",
      "17     0.2960       1.78         3198        women                                  \n",
      "18     0.2952       1.79         10973       Women                                  \n",
      "19     0.2934       1.81         52006      Queen                                   \n",
      "20     0.2908       1.78         52093       mujer                                  \n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nComputing Euclidean cosine similarities to ALL tokens...\\n\")\n",
    "\n",
    "# Compute cosine similarities\n",
    "norms = torch.norm(gamma, dim=1)\n",
    "norm_synthetic = torch.norm(v_synthetic)\n",
    "dot_products = gamma @ v_synthetic\n",
    "cosine_sims = (dot_products / (norms * norm_synthetic)).numpy()\n",
    "\n",
    "# Sort by cosine (descending)\n",
    "sorted_indices_cos = np.argsort(-cosine_sims)\n",
    "top_indices_cos = sorted_indices_cos[:TOP_N]\n",
    "\n",
    "print(f\"Top {TOP_N} tokens by EUCLIDEAN COSINE SIMILARITY:\")\n",
    "print(f\"{'Rank':<6} {'Cosine':<12} {'Distance':<12} {'Token ID':<10} {'Text':<40}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for rank, idx in enumerate(top_indices_cos, 1):\n",
    "    cos_sim = cosine_sims[idx]\n",
    "    dist = euclidean_distances[idx]\n",
    "    text = tokenizer.decode([idx])\n",
    "    print(f\"{rank:<6} {cos_sim:<12.4f} {dist:<12.2f} {idx:<10} {text:<40}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check for 'queen' Specifically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "CHECKING FOR 'queen' (token 93114):\n",
      "================================================================================\n",
      "\n",
      "Euclidean distance: 1.80 (rank 4496)\n",
      "Euclidean cosine: 0.3283 (rank 9)\n",
      "\n",
      "❌ 'queen' is rank 4496 by Euclidean distance\n",
      "   Word2vec arithmetic does NOT work in this space\n",
      "🎉 'queen' is in the TOP 10 by Euclidean cosine!\n"
     ]
    }
   ],
   "source": [
    "if TOKENS['queen'] is not None:\n",
    "    idx_queen = TOKENS['queen']\n",
    "    \n",
    "    dist_to_queen = euclidean_distances[idx_queen]\n",
    "    cos_to_queen = cosine_sims[idx_queen]\n",
    "    \n",
    "    # Find rank\n",
    "    rank_by_distance = np.where(sorted_indices == idx_queen)[0][0] + 1\n",
    "    rank_by_cosine = np.where(sorted_indices_cos == idx_queen)[0][0] + 1\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(f\"CHECKING FOR 'queen' (token {idx_queen}):\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"\\nEuclidean distance: {dist_to_queen:.2f} (rank {rank_by_distance})\")\n",
    "    print(f\"Euclidean cosine: {cos_to_queen:.4f} (rank {rank_by_cosine})\")\n",
    "    \n",
    "    if rank_by_distance <= 5:\n",
    "        print(f\"\\n🎉🎉🎉 'queen' is in the TOP 5 by Euclidean distance!\")\n",
    "        print(f\"         WORD2VEC MAGIC WORKS IN EUCLIDEAN SPACE!\")\n",
    "    elif rank_by_distance <= 10:\n",
    "        print(f\"\\n🎉 'queen' is in the TOP 10 by Euclidean distance!\")\n",
    "        print(f\"    Linear semantics confirmed!\")\n",
    "    elif rank_by_distance <= 20:\n",
    "        print(f\"\\n✓ 'queen' is in the top 20 by Euclidean distance\")\n",
    "        print(f\"  Moderate evidence for linear semantics\")\n",
    "    elif rank_by_distance <= 100:\n",
    "        print(f\"\\n'queen' is rank {rank_by_distance} by Euclidean distance\")\n",
    "        print(f\"  Weak signal, but detectable\")\n",
    "    else:\n",
    "        print(f\"\\n❌ 'queen' is rank {rank_by_distance} by Euclidean distance\")\n",
    "        print(f\"   Word2vec arithmetic does NOT work in this space\")\n",
    "    \n",
    "    if rank_by_cosine <= 5:\n",
    "        print(f\"\\n🎉🎉🎉 'queen' is in the TOP 5 by Euclidean cosine!\")\n",
    "    elif rank_by_cosine <= 10:\n",
    "        print(f\"🎉 'queen' is in the TOP 10 by Euclidean cosine!\")\n",
    "    elif rank_by_cosine <= 20:\n",
    "        print(f\"✓ 'queen' is in the top 20 by Euclidean cosine\")\n",
    "    elif rank_by_cosine <= 100:\n",
    "        print(f\"'queen' is rank {rank_by_cosine} by Euclidean cosine\")\n",
    "    else:\n",
    "        print(f\"❌ 'queen' is rank {rank_by_cosine} by Euclidean cosine\")\n",
    "else:\n",
    "    print(\"\\n⚠️  Token 'queen' not found or not a single token\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overlap Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "OVERLAP ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "Tokens in top 20 by BOTH metrics:\n",
      "  Count: 6\n",
      "\n",
      "Tokens appearing in both lists:\n",
      "  • 10566: 'king' (distance=1.30, cosine=0.6945)\n",
      "  • 11477: ' king' (distance=1.71, cosine=0.3806)\n",
      "  • 22028: 'woman' (distance=1.60, cosine=0.4840)\n",
      "  • 33555: 'King' (distance=1.69, cosine=0.4002)\n",
      "  • 64662: 'women' (distance=1.73, cosine=0.3771)\n",
      "  • 73811: ' KING' (distance=1.72, cosine=0.3839)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"OVERLAP ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Find tokens that appear in BOTH top N lists\n",
    "top_by_distance = set(sorted_indices[:TOP_N])\n",
    "top_by_cosine = set(sorted_indices_cos[:TOP_N])\n",
    "\n",
    "overlap = top_by_distance & top_by_cosine\n",
    "\n",
    "print(f\"\\nTokens in top {TOP_N} by BOTH metrics:\")\n",
    "print(f\"  Count: {len(overlap)}\")\n",
    "\n",
    "if overlap:\n",
    "    print(f\"\\nTokens appearing in both lists:\")\n",
    "    for tid in sorted(overlap):\n",
    "        text = tokenizer.decode([tid])\n",
    "        dist = euclidean_distances[tid]\n",
    "        cos = cosine_sims[tid]\n",
    "        print(f\"  • {tid}: '{text}' (distance={dist:.2f}, cosine={cos:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**What we tested:** The classic word2vec arithmetic `'woman' - 'man' + 'king' ≈ 'queen'` in **Euclidean geometry**.\n",
    "\n",
    "**Why this matters:** If this works, we know that:\n",
    "1. The raw unembedding matrix has linear semantic structure\n",
    "2. Compositional semantics are preserved in this LLM\n",
    "3. Word2vec magic transfers to modern transformers\n",
    "\n",
    "**Next step:** Compare with 07.59b (causal metric) to see if the metric preserves or destroys this structure."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "azimuth",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
