{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec Test: Causal Geometry\n",
    "\n",
    "**Goal:** Test the canonical word2vec arithmetic in **causal metric space**.\n",
    "\n",
    "**Hypothesis:** `'woman' - 'man' + 'king' ‚âà 'queen'` (measured with causal metric M)\n",
    "\n",
    "**Why this matters:** This tests whether the **causal metric** (Park et al. 2024) preserves linear semantic relationships. We compare with 07.59a (Euclidean) to see if the probability geometry helps or hurts compositional semantics.\n",
    "\n",
    "**Method:**\n",
    "1. Compute synthetic vector: `v = 'woman' - 'man' + 'king'`\n",
    "2. Find nearest neighbors by **causal distance** d_M(u,v) = sqrt((u-v)^T M (u-v))\n",
    "3. Find nearest neighbors by **causal cosine similarity** (u^T M v) / (||u||_M ||v||_M)\n",
    "4. Check where 'queen' ranks\n",
    "\n",
    "**Note:** We search the **full vocabulary** (151k tokens), not just our 32k sample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration:\n",
      "  Model: Qwen/Qwen3-4B-Instruct-2507\n",
      "  Arithmetic: 'woman' - 'man' + 'king' = ?\n",
      "  Metric: CAUSAL (using M tensor)\n",
      "  Search space: Full vocabulary (151k tokens)\n",
      "  Top N results: 20\n"
     ]
    }
   ],
   "source": [
    "# Model\n",
    "MODEL_NAME = 'Qwen/Qwen3-4B-Instruct-2507'\n",
    "\n",
    "# Data paths\n",
    "METRIC_TENSOR_PATH = '../data/vectors/causal_metric_tensor_qwen3_4b.pt'\n",
    "\n",
    "# Tokens for arithmetic\n",
    "TOKENS = {\n",
    "    'man': None,    # Will tokenize\n",
    "    'woman': None,  # Will tokenize\n",
    "    'king': None,   # Will tokenize\n",
    "    'queen': None,  # Will tokenize (for checking)\n",
    "}\n",
    "\n",
    "# Analysis parameters\n",
    "TOP_N = 20  # How many neighbors to show\n",
    "\n",
    "print(f\"Configuration:\")\n",
    "print(f\"  Model: {MODEL_NAME}\")\n",
    "print(f\"  Arithmetic: 'woman' - 'man' + 'king' = ?\")\n",
    "print(f\"  Metric: CAUSAL (using M tensor)\")\n",
    "print(f\"  Search space: Full vocabulary (151k tokens)\")\n",
    "print(f\"  Top N results: {TOP_N}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Imports complete\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "print(\"‚úì Imports complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model, Tokenizer, and Metric Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model, tokenizer, and metric tensor...\n",
      "\n",
      "Loading tokenizer from Qwen/Qwen3-4B-Instruct-2507...\n",
      "\n",
      "Loading model (for unembedding matrix)...\n",
      "  This will take a minute...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7caa293964924196bba6bcc01d3d070b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading causal metric tensor from ../data/vectors/causal_metric_tensor_qwen3_4b.pt...\n",
      "\n",
      "‚úì All data loaded\n",
      "  Vocab size: 151,643\n",
      "  Unembedding matrix shape: torch.Size([151936, 2560])\n",
      "  Metric tensor shape: torch.Size([2560, 2560])\n",
      "  Memory: 1.58 GB\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading model, tokenizer, and metric tensor...\\n\")\n",
    "\n",
    "# Tokenizer\n",
    "print(f\"Loading tokenizer from {MODEL_NAME}...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Model (for full unembedding matrix)\n",
    "print(f\"\\nLoading model (for unembedding matrix)...\")\n",
    "print(\"  This will take a minute...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map='cpu',\n",
    ")\n",
    "\n",
    "# Extract FULL unembedding matrix (all vocab)\n",
    "gamma = model.lm_head.weight.data.to(torch.float32).cpu()  # [vocab_size, hidden_dim]\n",
    "\n",
    "# Load metric tensor\n",
    "print(f\"\\nLoading causal metric tensor from {METRIC_TENSOR_PATH}...\")\n",
    "metric_data = torch.load(METRIC_TENSOR_PATH, weights_only=False)\n",
    "M = metric_data['M'].to(torch.float32).cpu()  # [hidden_dim, hidden_dim]\n",
    "\n",
    "print(f\"\\n‚úì All data loaded\")\n",
    "print(f\"  Vocab size: {tokenizer.vocab_size:,}\")\n",
    "print(f\"  Unembedding matrix shape: {gamma.shape}\")\n",
    "print(f\"  Metric tensor shape: {M.shape}\")\n",
    "print(f\"  Memory: {(gamma.element_size() * gamma.nelement() + M.element_size() * M.nelement()) / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize and Verify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing target words...\n",
      "\n",
      "‚úì 'man' ‚Üí token 1515: 'man'\n",
      "‚úì 'woman' ‚Üí token 22028: 'woman'\n",
      "‚úì 'king' ‚Üí token 10566: 'king'\n",
      "‚úì 'queen' ‚Üí token 93114: 'queen'\n",
      "\n",
      "‚úì All words are single tokens\n"
     ]
    }
   ],
   "source": [
    "print(\"Tokenizing target words...\\n\")\n",
    "\n",
    "words = ['man', 'woman', 'king', 'queen']\n",
    "all_single_tokens = True\n",
    "\n",
    "for word in words:\n",
    "    tokens = tokenizer.encode(word, add_special_tokens=False)\n",
    "    \n",
    "    if len(tokens) == 1:\n",
    "        token_id = tokens[0]\n",
    "        TOKENS[word] = token_id\n",
    "        text = tokenizer.decode([token_id])\n",
    "        print(f\"‚úì '{word}' ‚Üí token {token_id}: '{text}'\")\n",
    "    else:\n",
    "        print(f\"‚úó '{word}' ‚Üí {len(tokens)} tokens: {tokens}\")\n",
    "        all_single_tokens = False\n",
    "\n",
    "if not all_single_tokens:\n",
    "    print(\"\\n‚ö†Ô∏è  Not all words are single tokens!\")\n",
    "    print(\"    This may affect results, but we'll proceed anyway.\")\n",
    "else:\n",
    "    print(\"\\n‚úì All words are single tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Synthetic Vector: 'woman' - 'man' + 'king'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Computing synthetic vector...\n",
      "\n",
      "v_synthetic = 'woman' - 'man' + 'king'\n",
      "\n",
      "Synthetic vector properties:\n",
      "  Shape: torch.Size([2560])\n",
      "  Euclidean norm: 1.81\n",
      "  Causal norm: 86.75 logometers\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nComputing synthetic vector...\\n\")\n",
    "\n",
    "# Get embeddings\n",
    "v_man = gamma[TOKENS['man']]\n",
    "v_woman = gamma[TOKENS['woman']]\n",
    "v_king = gamma[TOKENS['king']]\n",
    "\n",
    "# Arithmetic\n",
    "v_synthetic = v_woman - v_man + v_king\n",
    "\n",
    "print(f\"v_synthetic = 'woman' - 'man' + 'king'\")\n",
    "print(f\"\\nSynthetic vector properties:\")\n",
    "print(f\"  Shape: {v_synthetic.shape}\")\n",
    "print(f\"  Euclidean norm: {torch.norm(v_synthetic).item():.2f}\")\n",
    "\n",
    "# Compute causal norm\n",
    "causal_norm_synthetic = torch.sqrt(v_synthetic @ M @ v_synthetic).item()\n",
    "print(f\"  Causal norm: {causal_norm_synthetic:.2f} logometers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Direct Vector Comparison: Are the Transformations Parallel?\n",
    "\n",
    "**Simple test:** Do `woman - man` and `queen - king` point in the same direction (in causal space)?\n",
    "\n",
    "If word2vec arithmetic works, these should be **parallel** under the causal metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "DIRECT VECTOR COMPARISON (CAUSAL METRIC)\n",
      "================================================================================\n",
      "\n",
      "(woman - man) properties:\n",
      "  Causal norm: 60.7513 logometers\n",
      "\n",
      "(queen - king) properties:\n",
      "  Causal norm: 68.3953 logometers\n",
      "\n",
      "Alignment between the two vectors (causal metric):\n",
      "  Causal dot product: 253.7651\n",
      "  Causal cosine similarity: 0.0611\n",
      "  Causal angle: 86.50¬∞\n",
      "\n",
      "================================================================================\n",
      "INTERPRETATION:\n",
      "================================================================================\n",
      "‚ùå Vectors are nearly ORTHOGONAL in causal space (cos=0.061, angle=86.5¬∞)\n",
      "   No meaningful linear relationship under the causal metric\n",
      "   Word2vec arithmetic will NOT work in causal geometry\n",
      "\n",
      "For reference:\n",
      "  ‚Ä¢ Parallel vectors: cos=1.0, angle=0¬∞\n",
      "  ‚Ä¢ Orthogonal vectors: cos=0.0, angle=90¬∞\n",
      "  ‚Ä¢ Opposite vectors: cos=-1.0, angle=180¬∞\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"DIRECT VECTOR COMPARISON (CAUSAL METRIC)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Get queen vector\n",
    "v_queen = gamma[TOKENS['queen']]\n",
    "\n",
    "# Compute the two displacement vectors\n",
    "v_gender_shift = v_woman - v_man      # man ‚Üí woman\n",
    "v_royalty_shift = v_queen - v_king    # king ‚Üí queen\n",
    "\n",
    "# Causal norms: ||v||_M = sqrt(v^T M v)\n",
    "causal_norm_gender = torch.sqrt(v_gender_shift @ M @ v_gender_shift).item()\n",
    "causal_norm_royalty = torch.sqrt(v_royalty_shift @ M @ v_royalty_shift).item()\n",
    "\n",
    "# Causal dot product and cosine: (u^T M v) / (||u||_M ||v||_M)\n",
    "causal_dot_product = (v_gender_shift @ M @ v_royalty_shift).item()\n",
    "causal_cosine_similarity = causal_dot_product / (causal_norm_gender * causal_norm_royalty)\n",
    "causal_angle_rad = np.arccos(np.clip(causal_cosine_similarity, -1.0, 1.0))\n",
    "causal_angle_deg = np.degrees(causal_angle_rad)\n",
    "\n",
    "print(f\"\\n(woman - man) properties:\")\n",
    "print(f\"  Causal norm: {causal_norm_gender:.4f} logometers\")\n",
    "\n",
    "print(f\"\\n(queen - king) properties:\")\n",
    "print(f\"  Causal norm: {causal_norm_royalty:.4f} logometers\")\n",
    "\n",
    "print(f\"\\nAlignment between the two vectors (causal metric):\")\n",
    "print(f\"  Causal dot product: {causal_dot_product:.4f}\")\n",
    "print(f\"  Causal cosine similarity: {causal_cosine_similarity:.4f}\")\n",
    "print(f\"  Causal angle: {causal_angle_deg:.2f}¬∞\")\n",
    "\n",
    "print(f\"\\n\" + \"=\" * 80)\n",
    "print(\"INTERPRETATION:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if abs(causal_cosine_similarity) > 0.9:\n",
    "    print(f\"‚úÖ Vectors are HIGHLY ALIGNED in causal space (cos={causal_cosine_similarity:.3f})\")\n",
    "    print(f\"   Word2vec arithmetic should work in causal geometry!\")\n",
    "elif abs(causal_cosine_similarity) > 0.7:\n",
    "    print(f\"‚úì Vectors are moderately aligned in causal space (cos={causal_cosine_similarity:.3f})\")\n",
    "    print(f\"  Some linear structure exists under the causal metric\")\n",
    "elif abs(causal_cosine_similarity) > 0.3:\n",
    "    print(f\"‚ö†Ô∏è  Vectors are weakly aligned in causal space (cos={causal_cosine_similarity:.3f})\")\n",
    "    print(f\"   Minimal linear relationship under the causal metric\")\n",
    "else:\n",
    "    print(f\"‚ùå Vectors are nearly ORTHOGONAL in causal space (cos={causal_cosine_similarity:.3f}, angle={causal_angle_deg:.1f}¬∞)\")\n",
    "    print(f\"   No meaningful linear relationship under the causal metric\")\n",
    "    print(f\"   Word2vec arithmetic will NOT work in causal geometry\")\n",
    "\n",
    "print(f\"\\nFor reference:\")\n",
    "print(f\"  ‚Ä¢ Parallel vectors: cos=1.0, angle=0¬∞\")\n",
    "print(f\"  ‚Ä¢ Orthogonal vectors: cos=0.0, angle=90¬∞\")\n",
    "print(f\"  ‚Ä¢ Opposite vectors: cos=-1.0, angle=180¬∞\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precompute Causal Norms for All Tokens\n",
    "\n",
    "We need these for cosine similarity calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Precomputing causal norms for all tokens...\n",
      "  (This may take a minute for 151k tokens...)\n",
      "\n",
      "‚úì Causal norms computed\n",
      "  Range: [21.35, 85.29] logometers\n",
      "  Mean: 54.13 logometers\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nPrecomputing causal norms for all tokens...\")\n",
    "print(\"  (This may take a minute for 151k tokens...)\\n\")\n",
    "\n",
    "# ||v||_M = sqrt(v^T M v)\n",
    "causal_norms = torch.sqrt(torch.sum(gamma @ M * gamma, dim=1)).numpy()\n",
    "\n",
    "print(f\"‚úì Causal norms computed\")\n",
    "print(f\"  Range: [{causal_norms.min():.2f}, {causal_norms.max():.2f}] logometers\")\n",
    "print(f\"  Mean: {causal_norms.mean():.2f} logometers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Scaled Vector Arithmetic (Causal Metric)\n",
    "\n",
    "**Question:** Is the failure due to **magnitude** or **direction** in causal space?\n",
    "\n",
    "**Hypothesis:** Maybe `queen = king + Œ± * (woman - man)` for some optimal Œ± ‚â† 1 (measured with M)\n",
    "\n",
    "**Method:** Solve for optimal Œ± via least-squares projection **in causal metric**:\n",
    "\n",
    "```\n",
    "Œ± = (queen - king)^T M (woman - man) / [(woman - man)^T M (woman - man)]\n",
    "```\n",
    "\n",
    "This finds the best scaling in the **causal geometry**, not Euclidean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Computing causal distances to ALL tokens in vocabulary...\n",
      "\n",
      "  (This may take a few minutes for 151k tokens...)\n",
      "\n",
      "  Progress: 0 / 151,936 tokens...\n",
      "  Progress: 10,000 / 151,936 tokens...\n",
      "  Progress: 20,000 / 151,936 tokens...\n",
      "  Progress: 30,000 / 151,936 tokens...\n",
      "  Progress: 40,000 / 151,936 tokens...\n",
      "  Progress: 50,000 / 151,936 tokens...\n",
      "  Progress: 60,000 / 151,936 tokens...\n",
      "  Progress: 70,000 / 151,936 tokens...\n",
      "  Progress: 80,000 / 151,936 tokens...\n",
      "  Progress: 90,000 / 151,936 tokens...\n",
      "  Progress: 100,000 / 151,936 tokens...\n",
      "  Progress: 110,000 / 151,936 tokens...\n",
      "  Progress: 120,000 / 151,936 tokens...\n",
      "  Progress: 130,000 / 151,936 tokens...\n",
      "  Progress: 140,000 / 151,936 tokens...\n",
      "  Progress: 150,000 / 151,936 tokens...\n",
      "\n",
      "‚úì Causal distances computed\n",
      "\n",
      "Top 20 tokens by CAUSAL DISTANCE:\n",
      "Rank   Distance     Token ID   Text                                    \n",
      "================================================================================\n",
      "1      60.75        10566      king                                    \n",
      "2      79.26        22028      woman                                   \n",
      "3      81.09        33555      King                                    \n",
      "4      81.89        11477       king                                   \n",
      "5      83.86        6210        King                                   \n",
      "6      84.48        93973      -vesm                                   \n",
      "7      84.48        150920     Î°û                                       \n",
      "8      84.50        143978     ◊û◊™◊®◊ó◊©                                   \n",
      "9      84.50        151288     ùÑ±                                       \n",
      "10     84.51        149802     Ìñ§                                       \n",
      "11     84.51        142835     „ÅäÂãß                                      \n",
      "12     84.51        151070     ÌÉπ                                       \n",
      "13     84.51        151444     Ïù©                                       \n",
      "14     84.52        148532     Ìô±                                       \n",
      "15     84.52        151099     Ìï∂                                       \n",
      "16     84.52        123302     °êì                                       \n",
      "17     84.52        151004     ÏòÇ                                       \n",
      "18     84.52        149077     Íπû                                       \n",
      "19     84.52        117035     ËÄ∂ÔøΩ                                      \n",
      "20     84.52        148550     Íª≠                                       \n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nComputing causal distances to ALL tokens in vocabulary...\\n\")\n",
    "print(\"  (This may take a few minutes for 151k tokens...)\\n\")\n",
    "\n",
    "# Compute causal distances: d_M(u, v) = sqrt((u-v)^T M (u-v))\n",
    "vocab_size = gamma.shape[0]\n",
    "causal_distances = np.zeros(vocab_size)\n",
    "\n",
    "for i in range(vocab_size):\n",
    "    if i % 10000 == 0:\n",
    "        print(f\"  Progress: {i:,} / {vocab_size:,} tokens...\")\n",
    "    diff = gamma[i] - v_synthetic\n",
    "    causal_distances[i] = torch.sqrt(diff @ M @ diff).item()\n",
    "\n",
    "print(f\"\\n‚úì Causal distances computed\")\n",
    "\n",
    "# Sort by distance (ascending)\n",
    "sorted_indices = np.argsort(causal_distances)\n",
    "top_indices = sorted_indices[:TOP_N]\n",
    "\n",
    "print(f\"\\nTop {TOP_N} tokens by CAUSAL DISTANCE:\")\n",
    "print(f\"{'Rank':<6} {'Distance':<12} {'Token ID':<10} {'Text':<40}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for rank, idx in enumerate(top_indices, 1):\n",
    "    dist = causal_distances[idx]\n",
    "    text = tokenizer.decode([idx])\n",
    "    print(f\"{rank:<6} {dist:<12.2f} {idx:<10} {text:<40}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find Nearest Neighbors by Causal Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Computing causal cosine similarities to ALL tokens...\n",
      "\n",
      "Top 20 tokens by CAUSAL COSINE SIMILARITY:\n",
      "Rank   Cosine       Distance     Token ID   Text                                    \n",
      "================================================================================\n",
      "1      0.7139       60.75        10566      king                                    \n",
      "2      0.4474       79.26        22028      woman                                   \n",
      "3      0.4105       81.09        33555      King                                    \n",
      "4      0.3983       81.89        11477       king                                   \n",
      "5      0.3612       83.86        6210        King                                   \n",
      "6      0.3440       84.82        64662      women                                   \n",
      "7      0.3440       85.14        73811       KING                                   \n",
      "8      0.3278       85.43        27906       queen                                  \n",
      "9      0.3227       84.65        3198        women                                  \n",
      "10     0.3176       86.02        25079       kingdom                                \n",
      "11     0.3096       86.93        44519       kings                                  \n",
      "12     0.3013       85.83        35090      Women                                   \n",
      "13     0.2998       85.73        5220        woman                                  \n",
      "14     0.2946       85.95        10973       Women                                  \n",
      "15     0.2908       88.97        107894     ÂõΩÁéã                                      \n",
      "16     0.2876       88.66        93114      queen                                   \n",
      "17     0.2751       88.88        95049      Woman                                   \n",
      "18     0.2723       88.22        16259       Queen                                  \n",
      "19     0.2695       88.57        52006      Queen                                   \n",
      "20     0.2647       89.06        95406       kingdoms                               \n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nComputing causal cosine similarities to ALL tokens...\\n\")\n",
    "\n",
    "# Compute causal cosine: cos(Œ∏_M) = (u^T M v) / (||u||_M ¬∑ ||v||_M)\n",
    "causal_dot_products = (gamma @ M @ v_synthetic).numpy()  # [vocab_size]\n",
    "causal_cosines = causal_dot_products / (causal_norms * causal_norm_synthetic)\n",
    "\n",
    "# Sort by cosine (descending)\n",
    "sorted_indices_cos = np.argsort(-causal_cosines)\n",
    "top_indices_cos = sorted_indices_cos[:TOP_N]\n",
    "\n",
    "print(f\"Top {TOP_N} tokens by CAUSAL COSINE SIMILARITY:\")\n",
    "print(f\"{'Rank':<6} {'Cosine':<12} {'Distance':<12} {'Token ID':<10} {'Text':<40}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for rank, idx in enumerate(top_indices_cos, 1):\n",
    "    cos_sim = causal_cosines[idx]\n",
    "    dist = causal_distances[idx]\n",
    "    text = tokenizer.decode([idx])\n",
    "    print(f\"{rank:<6} {cos_sim:<12.4f} {dist:<12.2f} {idx:<10} {text:<40}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check for 'queen' Specifically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "CHECKING FOR 'queen' (token 93114):\n",
      "================================================================================\n",
      "\n",
      "Causal distance: 88.66 logometers (rank 5860)\n",
      "Causal cosine: 0.2876 (rank 16)\n",
      "\n",
      "‚ùå 'queen' is rank 5860 by causal distance\n",
      "   Word2vec arithmetic does NOT work in causal space\n",
      "‚úì 'queen' is in the top 20 by causal cosine\n"
     ]
    }
   ],
   "source": [
    "if TOKENS['queen'] is not None:\n",
    "    idx_queen = TOKENS['queen']\n",
    "    \n",
    "    dist_to_queen = causal_distances[idx_queen]\n",
    "    cos_to_queen = causal_cosines[idx_queen]\n",
    "    \n",
    "    # Find rank\n",
    "    rank_by_distance = np.where(sorted_indices == idx_queen)[0][0] + 1\n",
    "    rank_by_cosine = np.where(sorted_indices_cos == idx_queen)[0][0] + 1\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(f\"CHECKING FOR 'queen' (token {idx_queen}):\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"\\nCausal distance: {dist_to_queen:.2f} logometers (rank {rank_by_distance})\")\n",
    "    print(f\"Causal cosine: {cos_to_queen:.4f} (rank {rank_by_cosine})\")\n",
    "    \n",
    "    if rank_by_distance <= 5:\n",
    "        print(f\"\\nüéâüéâüéâ 'queen' is in the TOP 5 by causal distance!\")\n",
    "        print(f\"         WORD2VEC MAGIC WORKS IN CAUSAL SPACE!\")\n",
    "    elif rank_by_distance <= 10:\n",
    "        print(f\"\\nüéâ 'queen' is in the TOP 10 by causal distance!\")\n",
    "        print(f\"    Linear semantics confirmed in causal geometry!\")\n",
    "    elif rank_by_distance <= 20:\n",
    "        print(f\"\\n‚úì 'queen' is in the top 20 by causal distance\")\n",
    "        print(f\"  Moderate evidence for linear semantics in causal space\")\n",
    "    elif rank_by_distance <= 100:\n",
    "        print(f\"\\n'queen' is rank {rank_by_distance} by causal distance\")\n",
    "        print(f\"  Weak signal, but detectable\")\n",
    "    else:\n",
    "        print(f\"\\n‚ùå 'queen' is rank {rank_by_distance} by causal distance\")\n",
    "        print(f\"   Word2vec arithmetic does NOT work in causal space\")\n",
    "    \n",
    "    if rank_by_cosine <= 5:\n",
    "        print(f\"\\nüéâüéâüéâ 'queen' is in the TOP 5 by causal cosine!\")\n",
    "    elif rank_by_cosine <= 10:\n",
    "        print(f\"üéâ 'queen' is in the TOP 10 by causal cosine!\")\n",
    "    elif rank_by_cosine <= 20:\n",
    "        print(f\"‚úì 'queen' is in the top 20 by causal cosine\")\n",
    "    elif rank_by_cosine <= 100:\n",
    "        print(f\"'queen' is rank {rank_by_cosine} by causal cosine\")\n",
    "    else:\n",
    "        print(f\"‚ùå 'queen' is rank {rank_by_cosine} by causal cosine\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  Token 'queen' not found or not a single token\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "TESTING SCALED VECTOR ARITHMETIC (CAUSAL METRIC)\n",
      "================================================================================\n",
      "\n",
      "Optimal scaling factor Œ± (causal) = 0.068758\n",
      "\n",
      "Interpretation:\n",
      "  Œ± = 0.07 ‚Üí Vectors are aligned but need rescaling\n",
      "  Partial compositional structure in causal space\n",
      "\n",
      "Method                         Distance (logo)      Angle (degrees)\n",
      "=================================================================\n",
      "Original (Œ±=1.0)               88.6629              73.29          \n",
      "Scaled (Œ±=0.0688)              68.2676              70.20          \n",
      "\n",
      "Distance improvement: 23.0%\n",
      "Angular improvement: 4.2%\n",
      "  ‚Üí Moderate distance improvement. Direction is partially correct\n"
     ]
    }
   ],
   "source": [
    "if TOKENS['queen'] is not None:\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"TESTING SCALED VECTOR ARITHMETIC (CAUSAL METRIC)\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Get queen embedding\n",
    "    v_queen = gamma[TOKENS['queen']]\n",
    "    \n",
    "    # Compute the gender shift vector\n",
    "    v_gender_shift = v_woman - v_man\n",
    "    \n",
    "    # Compute the target displacement\n",
    "    v_target_displacement = v_queen - v_king\n",
    "    \n",
    "    # Solve for Œ± via least-squares projection in CAUSAL metric\n",
    "    # Œ± = (target^T M shift) / (shift^T M shift)\n",
    "    numerator = (v_target_displacement @ M @ v_gender_shift).item()\n",
    "    denominator = (v_gender_shift @ M @ v_gender_shift).item()\n",
    "    alpha_optimal = numerator / denominator\n",
    "    \n",
    "    print(f\"\\nOptimal scaling factor Œ± (causal) = {alpha_optimal:.6f}\")\n",
    "    print(f\"\\nInterpretation:\")\n",
    "    if abs(alpha_optimal) < 0.01:\n",
    "        print(f\"  Œ± ‚âà 0 ‚Üí Gender shift is ORTHOGONAL to king‚Üíqueen in causal space\")\n",
    "        print(f\"  The vectors have no meaningful relationship\")\n",
    "    elif 0.8 <= alpha_optimal <= 1.2:\n",
    "        print(f\"  Œ± ‚âà 1 ‚Üí Gender shift aligns well with king‚Üíqueen in causal geometry!\")\n",
    "        print(f\"  Word2vec arithmetic works (just needed right magnitude)\")\n",
    "    else:\n",
    "        print(f\"  Œ± = {alpha_optimal:.2f} ‚Üí Vectors are aligned but need rescaling\")\n",
    "        print(f\"  Partial compositional structure in causal space\")\n",
    "    \n",
    "    # Construct the scaled synthetic vector\n",
    "    v_scaled = v_king + alpha_optimal * v_gender_shift\n",
    "    \n",
    "    # === CAUSAL DISTANCE ===\n",
    "    # Unscaled synthetic to queen (already computed)\n",
    "    causal_dist_synthetic_to_queen = causal_distances[TOKENS['queen']]\n",
    "    \n",
    "    # Scaled synthetic to queen\n",
    "    diff_scaled_queen = v_scaled - v_queen\n",
    "    causal_dist_scaled_to_queen = torch.sqrt(diff_scaled_queen @ M @ diff_scaled_queen).item()\n",
    "    \n",
    "    # === CAUSAL ANGLE ===\n",
    "    # Compute causal norms\n",
    "    causal_norm_synthetic = torch.sqrt(v_synthetic @ M @ v_synthetic).item()\n",
    "    causal_norm_scaled = torch.sqrt(v_scaled @ M @ v_scaled).item()\n",
    "    causal_norm_queen = causal_norms[TOKENS['queen']]\n",
    "    \n",
    "    # Compute causal cosine and convert to degrees\n",
    "    causal_dot_synthetic_queen = (v_synthetic @ M @ v_queen).item()\n",
    "    cos_causal_synthetic_queen = causal_dot_synthetic_queen / (causal_norm_synthetic * causal_norm_queen)\n",
    "    angle_causal_synthetic_queen_rad = np.arccos(np.clip(cos_causal_synthetic_queen, -1.0, 1.0))\n",
    "    angle_causal_synthetic_queen_deg = np.degrees(angle_causal_synthetic_queen_rad)\n",
    "    \n",
    "    causal_dot_scaled_queen = (v_scaled @ M @ v_queen).item()\n",
    "    cos_causal_scaled_queen = causal_dot_scaled_queen / (causal_norm_scaled * causal_norm_queen)\n",
    "    angle_causal_scaled_queen_rad = np.arccos(np.clip(cos_causal_scaled_queen, -1.0, 1.0))\n",
    "    angle_causal_scaled_queen_deg = np.degrees(angle_causal_scaled_queen_rad)\n",
    "    \n",
    "    print(f\"\\n{'Method':<30} {'Distance (logo)':<20} {'Angle (degrees)':<15}\")\n",
    "    print(\"=\" * 65)\n",
    "    print(f\"{'Original (Œ±=1.0)':<30} {causal_dist_synthetic_to_queen:<20.4f} {angle_causal_synthetic_queen_deg:<15.2f}\")\n",
    "    print(f\"{'Scaled (Œ±=' + f'{alpha_optimal:.4f})':<30} {causal_dist_scaled_to_queen:<20.4f} {angle_causal_scaled_queen_deg:<15.2f}\")\n",
    "    \n",
    "    dist_improvement = (1 - causal_dist_scaled_to_queen / causal_dist_synthetic_to_queen) * 100\n",
    "    angle_improvement = (1 - angle_causal_scaled_queen_deg / angle_causal_synthetic_queen_deg) * 100\n",
    "    \n",
    "    print(f\"\\nDistance improvement: {dist_improvement:.1f}%\")\n",
    "    print(f\"Angular improvement: {angle_improvement:.1f}%\")\n",
    "    \n",
    "    if dist_improvement > 50:\n",
    "        print(\"  ‚Üí LARGE distance improvement! The issue was magnitude, not direction\")\n",
    "    elif dist_improvement > 10:\n",
    "        print(\"  ‚Üí Moderate distance improvement. Direction is partially correct\")\n",
    "    else:\n",
    "        print(\"  ‚Üí Minimal distance improvement. The vectors are not aligned in causal space\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overlap Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "OVERLAP ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "Tokens in top 20 by BOTH metrics:\n",
      "  Count: 5\n",
      "\n",
      "Tokens appearing in both lists:\n",
      "  ‚Ä¢ 6210: ' King' (distance=83.86, cosine=0.3612)\n",
      "  ‚Ä¢ 10566: 'king' (distance=60.75, cosine=0.7139)\n",
      "  ‚Ä¢ 11477: ' king' (distance=81.89, cosine=0.3983)\n",
      "  ‚Ä¢ 22028: 'woman' (distance=79.26, cosine=0.4474)\n",
      "  ‚Ä¢ 33555: 'King' (distance=81.09, cosine=0.4105)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"OVERLAP ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Find tokens that appear in BOTH top N lists\n",
    "top_by_distance = set(sorted_indices[:TOP_N])\n",
    "top_by_cosine = set(sorted_indices_cos[:TOP_N])\n",
    "\n",
    "overlap = top_by_distance & top_by_cosine\n",
    "\n",
    "print(f\"\\nTokens in top {TOP_N} by BOTH metrics:\")\n",
    "print(f\"  Count: {len(overlap)}\")\n",
    "\n",
    "if overlap:\n",
    "    print(f\"\\nTokens appearing in both lists:\")\n",
    "    for tid in sorted(overlap):\n",
    "        text = tokenizer.decode([tid])\n",
    "        dist = causal_distances[tid]\n",
    "        cos = causal_cosines[tid]\n",
    "        print(f\"  ‚Ä¢ {tid}: '{text}' (distance={dist:.2f}, cosine={cos:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**What we tested:** The classic word2vec arithmetic `'woman' - 'man' + 'king' ‚âà 'queen'` in **causal metric geometry**.\n",
    "\n",
    "**Why this matters:** \n",
    "- If it works here but not in Euclidean (07.59a), the causal metric **reveals** hidden semantic structure\n",
    "- If it works in Euclidean but not here, the causal metric **destroys** linear semantics\n",
    "- If it works in both, the causal metric **preserves** compositional structure\n",
    "- If it works in neither, unembedding space lacks linear semantic relationships\n",
    "\n",
    "**Comparison:** Check 07.59a results to determine which scenario holds."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "azimuth",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
