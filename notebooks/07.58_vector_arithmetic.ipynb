{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector Arithmetic: Testing Cross-Lingual Compositionality\n",
    "\n",
    "**Goal:** Test if semantic vector arithmetic works in causal geometry across languages.\n",
    "\n",
    "**Hypothesis:** If `'vier' - 'four'` captures \"German-ness\", then:\n",
    "```\n",
    "'vier' - 'four' + 'planet' ‚âà German word for planet\n",
    "```\n",
    "\n",
    "**Best case:** We get 'Planet' or German astronomy terms\n",
    "\n",
    "**Why this matters:** The linear representation hypothesis predicts that semantic relationships should be compositional‚Äîwe can add and subtract meaning vectors. If this works across languages in causal geometry, it's strong evidence that the model learned language-independent semantic directions.\n",
    "\n",
    "**Method:**\n",
    "1. Compute synthetic vector: `v = 'vier' - 'four' + 'planet'`\n",
    "2. Find nearest neighbors by causal distance\n",
    "3. Find nearest neighbors by causal cosine similarity\n",
    "4. Decode and analyze results\n",
    "\n",
    "**Inputs:**\n",
    "- Token embeddings from 07.57\n",
    "- Known tokens: 'four' (34024), 'planet' (50074)\n",
    "- Target token: 'vier' (57093) from German"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration:\n",
      "  Model: Qwen/Qwen3-4B-Instruct-2507\n",
      "  Arithmetic: 'vier' - 'four' + 'planet' = ?\n",
      "  Top N results: 20\n"
     ]
    }
   ],
   "source": [
    "# Model\n",
    "MODEL_NAME = 'Qwen/Qwen3-4B-Instruct-2507'\n",
    "\n",
    "# Data paths\n",
    "DISTANCES_PATH = '../data/vectors/distances_causal_32000_full.npy'\n",
    "METADATA_PATH = '../data/vectors/distances_causal_32000.pt'\n",
    "METRIC_TENSOR_PATH = '../data/vectors/causal_metric_tensor_qwen3_4b.pt'\n",
    "\n",
    "# Tokens for arithmetic\n",
    "TOKENS = {\n",
    "    'four': 34024,      # English number\n",
    "    'planet': 50074,    # English astronomy\n",
    "    'vier': 57093,      # German number (from 07.57 analysis)\n",
    "}\n",
    "\n",
    "# Analysis parameters\n",
    "TOP_N = 20  # How many neighbors to show\n",
    "\n",
    "print(f\"Configuration:\")\n",
    "print(f\"  Model: {MODEL_NAME}\")\n",
    "print(f\"  Arithmetic: 'vier' - 'four' + 'planet' = ?\")\n",
    "print(f\"  Top N results: {TOP_N}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Imports complete\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"‚úì Imports complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "\n",
      "Loading metadata from ../data/vectors/distances_causal_32000.pt...\n",
      "\n",
      "Loading tokenizer from Qwen/Qwen3-4B-Instruct-2507...\n",
      "\n",
      "Loading model (for unembedding matrix)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9797bd0a33f34ca79753fedc881493cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading causal metric tensor from ../data/vectors/causal_metric_tensor_qwen3_4b.pt...\n",
      "\n",
      "‚úì All data loaded\n",
      "  N tokens in sample: 32,000\n",
      "  Embedding dimension: 2560\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading data...\\n\")\n",
    "\n",
    "# Metadata and token indices\n",
    "print(f\"Loading metadata from {METADATA_PATH}...\")\n",
    "metadata = torch.load(METADATA_PATH, weights_only=False)\n",
    "token_indices = metadata['token_indices'].numpy()\n",
    "token_to_idx = {tid: idx for idx, tid in enumerate(token_indices)}\n",
    "\n",
    "# Tokenizer\n",
    "print(f\"\\nLoading tokenizer from {MODEL_NAME}...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Model (for embeddings)\n",
    "print(f\"\\nLoading model (for unembedding matrix)...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map='cpu',\n",
    ")\n",
    "gamma = model.lm_head.weight.data.to(torch.float32).cpu()\n",
    "sampled_embeddings = gamma[token_indices]\n",
    "\n",
    "# Metric tensor\n",
    "print(f\"\\nLoading causal metric tensor from {METRIC_TENSOR_PATH}...\")\n",
    "metric_data = torch.load(METRIC_TENSOR_PATH, weights_only=False)\n",
    "M = metric_data['M'].to(torch.float32).cpu()\n",
    "\n",
    "print(f\"\\n‚úì All data loaded\")\n",
    "print(f\"  N tokens in sample: {len(token_indices):,}\")\n",
    "print(f\"  Embedding dimension: {sampled_embeddings.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify Tokens Are In Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking if required tokens are in our sample...\n",
      "\n",
      "‚úì 'four' (token 34024, index 27606): 'four'\n",
      "‚úì 'planet' (token 50074, index 20043): 'planet'\n",
      "‚úì 'vier' (token 57093, index 16750): ' vier'\n",
      "\n",
      "‚úì All required tokens present\n"
     ]
    }
   ],
   "source": [
    "print(\"Checking if required tokens are in our sample...\\n\")\n",
    "\n",
    "all_present = True\n",
    "for name, token_id in TOKENS.items():\n",
    "    if token_id in token_to_idx:\n",
    "        idx = token_to_idx[token_id]\n",
    "        text = tokenizer.decode([token_id])\n",
    "        print(f\"‚úì '{name}' (token {token_id}, index {idx}): '{text}'\")\n",
    "    else:\n",
    "        print(f\"‚ùå '{name}' (token {token_id}) NOT IN SAMPLE\")\n",
    "        all_present = False\n",
    "\n",
    "if not all_present:\n",
    "    raise ValueError(\"Some required tokens are not in the sample!\")\n",
    "\n",
    "print(\"\\n‚úì All required tokens present\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Synthetic Vector: 'vier' - 'four' + 'planet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing synthetic vector...\n",
      "\n",
      "v_synthetic = 'vier' - 'four' + 'planet'\n",
      "\n",
      "Synthetic vector properties:\n",
      "  Shape: torch.Size([2560])\n",
      "  Euclidean norm: 1.62\n",
      "  Causal norm: 79.53 logometers\n"
     ]
    }
   ],
   "source": [
    "print(\"Computing synthetic vector...\\n\")\n",
    "\n",
    "# Get embeddings\n",
    "idx_vier = token_to_idx[TOKENS['vier']]\n",
    "idx_four = token_to_idx[TOKENS['four']]\n",
    "idx_planet = token_to_idx[TOKENS['planet']]\n",
    "\n",
    "v_vier = sampled_embeddings[idx_vier]\n",
    "v_four = sampled_embeddings[idx_four]\n",
    "v_planet = sampled_embeddings[idx_planet]\n",
    "\n",
    "# Arithmetic\n",
    "v_synthetic = v_vier - v_four + v_planet\n",
    "\n",
    "print(f\"v_synthetic = 'vier' - 'four' + 'planet'\")\n",
    "print(f\"\\nSynthetic vector properties:\")\n",
    "print(f\"  Shape: {v_synthetic.shape}\")\n",
    "print(f\"  Euclidean norm: {torch.norm(v_synthetic).item():.2f}\")\n",
    "\n",
    "# Compute causal norm of synthetic vector\n",
    "causal_norm_synthetic = torch.sqrt(v_synthetic @ M @ v_synthetic).item()\n",
    "print(f\"  Causal norm: {causal_norm_synthetic:.2f} logometers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find Nearest Neighbors by Causal Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Computing causal distances to all tokens...\n",
      "\n",
      "Top 20 tokens by CAUSAL DISTANCE:\n",
      "Rank   Distance     Token ID   Text                                    \n",
      "================================================================================\n",
      "1      54.55        50074      planet                                  \n",
      "2      70.09        59634      Planet                                  \n",
      "3      74.09        57093       vier                                   \n",
      "4      76.59        150187     „ã©                                       \n",
      "5      76.64        122768     ‰ì´                                       \n",
      "6      76.66        149829     Ô≠µ                                       \n",
      "7      76.67        150219     „ãØ                                       \n",
      "8      76.67        146965     Í°í                                       \n",
      "9      76.67        151036     Ï©ª                                       \n",
      "10     76.68        151114     Ìùü                                       \n",
      "11     76.69        149731     Íæà                                       \n",
      "12     76.69        149798     ÌÅâ                                       \n",
      "13     76.69        150877     ÎÉµ                                       \n",
      "14     76.69        149413     Ïπï                                       \n",
      "15     76.69        151226     Ô≥á                                       \n",
      "16     76.69        149577     ·ó≠                                       \n",
      "17     76.69        123072     ÔøΩ                                       \n",
      "18     76.69        150737     ‚≠û                                       \n",
      "19     76.69        150308     Ïìª                                       \n",
      "20     76.70        79269       ForCanBeConverted                      \n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nComputing causal distances to all tokens...\\n\")\n",
    "\n",
    "# Compute causal distances: d_M(u, v) = sqrt((u-v)^T M (u-v))\n",
    "N = len(token_indices)\n",
    "causal_distances = np.zeros(N)\n",
    "\n",
    "for i in range(N):\n",
    "    diff = sampled_embeddings[i] - v_synthetic\n",
    "    causal_distances[i] = torch.sqrt(diff @ M @ diff).item()\n",
    "\n",
    "# Sort by distance (ascending)\n",
    "sorted_indices = np.argsort(causal_distances)\n",
    "top_indices = sorted_indices[:TOP_N]\n",
    "\n",
    "print(f\"Top {TOP_N} tokens by CAUSAL DISTANCE:\")\n",
    "print(f\"{'Rank':<6} {'Distance':<12} {'Token ID':<10} {'Text':<40}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for rank, idx in enumerate(top_indices, 1):\n",
    "    dist = causal_distances[idx]\n",
    "    tid = token_indices[idx]\n",
    "    text = tokenizer.decode([tid])\n",
    "    print(f\"{rank:<6} {dist:<12.2f} {tid:<10} {text:<40}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find Nearest Neighbors by Causal Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Computing causal cosine similarities to all tokens...\n",
      "\n",
      "Top 20 tokens by CAUSAL COSINE SIMILARITY:\n",
      "Rank   Cosine       Distance     Token ID   Text                                    \n",
      "================================================================================\n",
      "1      0.7277       54.55        50074      planet                                  \n",
      "2      0.5110       70.09        59634      Planet                                  \n",
      "3      0.4429       74.09        57093       vier                                   \n",
      "4      0.2699       76.59        150187     „ã©                                       \n",
      "5      0.2673       76.66        149829     Ô≠µ                                       \n",
      "6      0.2671       76.64        122768     ‰ì´                                       \n",
      "7      0.2667       76.67        150219     „ãØ                                       \n",
      "8      0.2658       76.67        146965     Í°í                                       \n",
      "9      0.2657       76.68        151114     Ìùü                                       \n",
      "10     0.2656       76.67        151036     Ï©ª                                       \n",
      "11     0.2651       76.69        149731     Íæà                                       \n",
      "12     0.2651       76.69        151226     Ô≥á                                       \n",
      "13     0.2651       76.69        149798     ÌÅâ                                       \n",
      "14     0.2651       76.69        150877     ÎÉµ                                       \n",
      "15     0.2651       76.69        149413     Ïπï                                       \n",
      "16     0.2650       76.69        149577     ·ó≠                                       \n",
      "17     0.2650       76.69        150737     ‚≠û                                       \n",
      "18     0.2649       76.69        123072     ÔøΩ                                       \n",
      "19     0.2649       76.69        150308     Ïìª                                       \n",
      "20     0.2648       76.70        79269       ForCanBeConverted                      \n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nComputing causal cosine similarities to all tokens...\\n\")\n",
    "\n",
    "# Compute causal cosine: cos(Œ∏_M) = (u^T M v) / (||u||_M ¬∑ ||v||_M)\n",
    "causal_dot_products = sampled_embeddings @ M @ v_synthetic  # [N]\n",
    "\n",
    "# Compute causal norms for all tokens\n",
    "causal_norms = torch.sqrt(torch.sum(sampled_embeddings @ M * sampled_embeddings, dim=1)).numpy()\n",
    "\n",
    "# Cosine similarities\n",
    "causal_cosines = (causal_dot_products / (torch.tensor(causal_norms) * causal_norm_synthetic)).numpy()\n",
    "\n",
    "# Sort by cosine (descending)\n",
    "sorted_indices_cos = np.argsort(-causal_cosines)\n",
    "top_indices_cos = sorted_indices_cos[:TOP_N]\n",
    "\n",
    "print(f\"Top {TOP_N} tokens by CAUSAL COSINE SIMILARITY:\")\n",
    "print(f\"{'Rank':<6} {'Cosine':<12} {'Distance':<12} {'Token ID':<10} {'Text':<40}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for rank, idx in enumerate(top_indices_cos, 1):\n",
    "    cos_sim = causal_cosines[idx]\n",
    "    dist = causal_distances[idx]\n",
    "    tid = token_indices[idx]\n",
    "    text = tokenizer.decode([tid])\n",
    "    print(f\"{rank:<6} {cos_sim:<12.4f} {dist:<12.2f} {tid:<10} {text:<40}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check for 'Planet' Specifically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "CHECKING FOR 'Planet' (token 59634):\n",
      "================================================================================\n",
      "\n",
      "Causal distance: 70.09 logometers (rank 2)\n",
      "Causal cosine: 0.5110 (rank 2)\n",
      "\n",
      "üéâ 'Planet' is in the TOP 10 by causal distance!\n",
      "üéâ 'Planet' is in the TOP 10 by causal cosine!\n"
     ]
    }
   ],
   "source": [
    "# Token ID for 'Planet' (capitalized, from 07.57)\n",
    "PLANET_CAPITALIZED = 59634\n",
    "\n",
    "if PLANET_CAPITALIZED in token_to_idx:\n",
    "    idx_Planet = token_to_idx[PLANET_CAPITALIZED]\n",
    "    \n",
    "    dist_to_Planet = causal_distances[idx_Planet]\n",
    "    cos_to_Planet = causal_cosines[idx_Planet]\n",
    "    \n",
    "    # Find rank\n",
    "    rank_by_distance = np.where(sorted_indices == idx_Planet)[0][0] + 1\n",
    "    rank_by_cosine = np.where(sorted_indices_cos == idx_Planet)[0][0] + 1\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"CHECKING FOR 'Planet' (token 59634):\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"\\nCausal distance: {dist_to_Planet:.2f} logometers (rank {rank_by_distance})\")\n",
    "    print(f\"Causal cosine: {cos_to_Planet:.4f} (rank {rank_by_cosine})\")\n",
    "    \n",
    "    if rank_by_distance <= 10:\n",
    "        print(f\"\\nüéâ 'Planet' is in the TOP 10 by causal distance!\")\n",
    "    elif rank_by_distance <= 20:\n",
    "        print(f\"\\n‚úì 'Planet' is in the top 20 by causal distance\")\n",
    "    else:\n",
    "        print(f\"\\n'Planet' is rank {rank_by_distance} by causal distance\")\n",
    "    \n",
    "    if rank_by_cosine <= 10:\n",
    "        print(f\"üéâ 'Planet' is in the TOP 10 by causal cosine!\")\n",
    "    elif rank_by_cosine <= 20:\n",
    "        print(f\"‚úì 'Planet' is in the top 20 by causal cosine\")\n",
    "    else:\n",
    "        print(f\"'Planet' is rank {rank_by_cosine} by causal cosine\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  Token 'Planet' (59634) not in our sample\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overlap Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "OVERLAP ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "Tokens in top 20 by BOTH metrics:\n",
      "  Count: 20\n",
      "\n",
      "Tokens appearing in both lists:\n",
      "  ‚Ä¢ 50074: 'planet' (distance=54.55, cosine=0.7277)\n",
      "  ‚Ä¢ 57093: ' vier' (distance=74.09, cosine=0.4429)\n",
      "  ‚Ä¢ 59634: 'Planet' (distance=70.09, cosine=0.5110)\n",
      "  ‚Ä¢ 79269: ' ForCanBeConverted' (distance=76.70, cosine=0.2648)\n",
      "  ‚Ä¢ 122768: '‰ì´' (distance=76.64, cosine=0.2671)\n",
      "  ‚Ä¢ 123072: 'ÔøΩ' (distance=76.69, cosine=0.2649)\n",
      "  ‚Ä¢ 146965: 'Í°í' (distance=76.67, cosine=0.2658)\n",
      "  ‚Ä¢ 149413: 'Ïπï' (distance=76.69, cosine=0.2651)\n",
      "  ‚Ä¢ 149577: '·ó≠' (distance=76.69, cosine=0.2650)\n",
      "  ‚Ä¢ 149731: 'Íæà' (distance=76.69, cosine=0.2651)\n",
      "  ‚Ä¢ 149798: 'ÌÅâ' (distance=76.69, cosine=0.2651)\n",
      "  ‚Ä¢ 149829: 'Ô≠µ' (distance=76.66, cosine=0.2673)\n",
      "  ‚Ä¢ 150187: '„ã©' (distance=76.59, cosine=0.2699)\n",
      "  ‚Ä¢ 150219: '„ãØ' (distance=76.67, cosine=0.2667)\n",
      "  ‚Ä¢ 150308: 'Ïìª' (distance=76.69, cosine=0.2649)\n",
      "  ‚Ä¢ 150737: '‚≠û' (distance=76.69, cosine=0.2650)\n",
      "  ‚Ä¢ 150877: 'ÎÉµ' (distance=76.69, cosine=0.2651)\n",
      "  ‚Ä¢ 151036: 'Ï©ª' (distance=76.67, cosine=0.2656)\n",
      "  ‚Ä¢ 151114: 'Ìùü' (distance=76.68, cosine=0.2657)\n",
      "  ‚Ä¢ 151226: 'Ô≥á' (distance=76.69, cosine=0.2651)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"OVERLAP ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Find tokens that appear in BOTH top N lists\n",
    "top_by_distance = set(token_indices[top_indices])\n",
    "top_by_cosine = set(token_indices[top_indices_cos])\n",
    "\n",
    "overlap = top_by_distance & top_by_cosine\n",
    "\n",
    "print(f\"\\nTokens in top {TOP_N} by BOTH metrics:\")\n",
    "print(f\"  Count: {len(overlap)}\")\n",
    "\n",
    "if overlap:\n",
    "    print(f\"\\nTokens appearing in both lists:\")\n",
    "    for tid in sorted(overlap):\n",
    "        text = tokenizer.decode([tid])\n",
    "        idx = token_to_idx[tid]\n",
    "        dist = causal_distances[idx]\n",
    "        cos = causal_cosines[idx]\n",
    "        print(f\"  ‚Ä¢ {tid}: '{text}' (distance={dist:.2f}, cosine={cos:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretation\n",
    "\n",
    "**What we're looking for:**\n",
    "\n",
    "1. **Best case:** 'Planet' (German capitalized) appears in top results\n",
    "   - Would confirm: `'vier' - 'four' + 'planet' ‚âà 'Planet'`\n",
    "   - Evidence: Language shift vector works compositionally\n",
    "\n",
    "2. **Good case:** German words appear in top results (even if not astronomy-related)\n",
    "   - Would suggest: `'vier' - 'four'` captures some \"German-ness\"\n",
    "   - But: Language vector might not transfer perfectly across semantic domains\n",
    "\n",
    "3. **Moderate case:** Astronomy words appear (but not necessarily German)\n",
    "   - Would suggest: 'planet' dominates the arithmetic\n",
    "   - Language shift vector too weak or context-dependent\n",
    "\n",
    "4. **Worst case:** Random tokens or neither German nor astronomy\n",
    "   - Would suggest: Vector arithmetic doesn't compose in this space\n",
    "   - Linear representation hypothesis fails for cross-lingual transfer\n",
    "\n",
    "**Key metrics:**\n",
    "- If 'Planet' is in top 10 by distance ‚Üí **strong evidence for compositionality**\n",
    "- If 'Planet' is in top 20 ‚Üí **moderate evidence**\n",
    "- If 'Planet' appears in both top-20 lists ‚Üí **very strong evidence**\n",
    "\n",
    "---\n",
    "\n",
    "## Results Analysis\n",
    "\n",
    "**What we found:**\n",
    "- 'Planet' is **#2** by both causal distance (70.09 logometers) and causal cosine (0.51)\n",
    "- This is **farther** than the original distance from 'planet' to 'Planet' (43.56 logometers, cosine 0.71)\n",
    "\n",
    "**Conclusion:** The unscaled arithmetic `'vier' - 'four' + 'planet'` moved us **away** from 'Planet', not toward it.\n",
    "\n",
    "**Question:** What if we need to **scale** the language shift vector?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Scaled Vector Arithmetic\n",
    "\n",
    "**Hypothesis:** Maybe we need to scale the language shift vector:\n",
    "\n",
    "```\n",
    "v_Planet ‚âà v_planet + Œ± * (v_vier - v_four)\n",
    "```\n",
    "\n",
    "**Method:** Solve for the optimal Œ± via least-squares projection:\n",
    "\n",
    "```\n",
    "Œ± = (v_Planet - v_planet) ¬∑ (v_vier - v_four) / ||v_vier - v_four||¬≤\n",
    "```\n",
    "\n",
    "Then test if the scaled version gets us closer to 'Planet'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "TESTING SCALED VECTOR ARITHMETIC\n",
      "================================================================================\n",
      "\n",
      "Optimal scaling factor Œ± = -0.0004\n",
      "\n",
      "This means: v_Planet ‚âà v_planet + -0.0004 * (v_vier - v_four)\n",
      "\n",
      "Scaled synthetic vector properties:\n",
      "  Euclidean norm: 1.17\n",
      "  Causal norm: 58.35 logometers\n",
      "\n",
      "================================================================================\n",
      "COMPARISON: Unscaled vs Scaled vs Original\n",
      "================================================================================\n",
      "\n",
      "1. Original 'planet' to 'Planet':\n",
      "   Distance: 43.56 logometers\n",
      "   Cosine: 0.71\n",
      "\n",
      "2. Unscaled synthetic (Œ±=1) to 'Planet':\n",
      "   Distance: 70.09 logometers\n",
      "   Cosine: 0.51\n",
      "\n",
      "3. SCALED synthetic (Œ±=-0.0004) to 'Planet':\n",
      "   Distance: 43.55 logometers\n",
      "   Cosine: 0.7110\n",
      "\n",
      "================================================================================\n",
      "‚úÖ SCALED VERSION IS CLOSER by 0.01 logometers!\n",
      "‚úÖ SCALED VERSION IS MORE ALIGNED by 0.0010!\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"TESTING SCALED VECTOR ARITHMETIC\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Get 'Planet' embedding\n",
    "idx_Planet = token_to_idx[PLANET_CAPITALIZED]\n",
    "v_Planet = sampled_embeddings[idx_Planet]\n",
    "\n",
    "# Compute the language shift vector\n",
    "v_shift = v_vier - v_four\n",
    "\n",
    "# Compute the target displacement\n",
    "v_target_displacement = v_Planet - v_planet\n",
    "\n",
    "# Solve for Œ± via least-squares projection\n",
    "# Œ± = (target ¬∑ shift) / (shift ¬∑ shift)\n",
    "numerator = torch.dot(v_target_displacement, v_shift).item()\n",
    "denominator = torch.dot(v_shift, v_shift).item()\n",
    "alpha_optimal = numerator / denominator\n",
    "\n",
    "print(f\"\\nOptimal scaling factor Œ± = {alpha_optimal:.4f}\")\n",
    "print(f\"\\nThis means: v_Planet ‚âà v_planet + {alpha_optimal:.4f} * (v_vier - v_four)\")\n",
    "\n",
    "# Construct the scaled synthetic vector\n",
    "v_scaled = v_planet + alpha_optimal * v_shift\n",
    "\n",
    "print(f\"\\nScaled synthetic vector properties:\")\n",
    "print(f\"  Euclidean norm: {torch.norm(v_scaled).item():.2f}\")\n",
    "\n",
    "causal_norm_scaled = torch.sqrt(v_scaled @ M @ v_scaled).item()\n",
    "print(f\"  Causal norm: {causal_norm_scaled:.2f} logometers\")\n",
    "\n",
    "# Compute distance from scaled vector to 'Planet'\n",
    "diff_to_Planet = v_scaled - v_Planet\n",
    "dist_scaled_to_Planet = torch.sqrt(diff_to_Planet @ M @ diff_to_Planet).item()\n",
    "\n",
    "# Compute cosine similarity\n",
    "causal_norm_Planet = causal_norms[idx_Planet]\n",
    "causal_dot_scaled_Planet = (v_scaled @ M @ v_Planet).item()\n",
    "cos_scaled_to_Planet = causal_dot_scaled_Planet / (causal_norm_scaled * causal_norm_Planet)\n",
    "\n",
    "print(f\"\\n{'=' * 80}\")\n",
    "print(f\"COMPARISON: Unscaled vs Scaled vs Original\")\n",
    "print(f\"{'=' * 80}\")\n",
    "\n",
    "print(f\"\\n1. Original 'planet' to 'Planet':\")\n",
    "print(f\"   Distance: 43.56 logometers\")\n",
    "print(f\"   Cosine: 0.71\")\n",
    "\n",
    "print(f\"\\n2. Unscaled synthetic (Œ±=1) to 'Planet':\")\n",
    "print(f\"   Distance: 70.09 logometers\")\n",
    "print(f\"   Cosine: 0.51\")\n",
    "\n",
    "print(f\"\\n3. SCALED synthetic (Œ±={alpha_optimal:.4f}) to 'Planet':\")\n",
    "print(f\"   Distance: {dist_scaled_to_Planet:.2f} logometers\")\n",
    "print(f\"   Cosine: {cos_scaled_to_Planet:.4f}\")\n",
    "\n",
    "print(f\"\\n{'=' * 80}\")\n",
    "\n",
    "# Determine outcome\n",
    "if dist_scaled_to_Planet < 43.56:\n",
    "    improvement_dist = 43.56 - dist_scaled_to_Planet\n",
    "    print(f\"‚úÖ SCALED VERSION IS CLOSER by {improvement_dist:.2f} logometers!\")\n",
    "elif dist_scaled_to_Planet < 70.09:\n",
    "    improvement_dist = 70.09 - dist_scaled_to_Planet\n",
    "    print(f\"‚úì Scaled version is better than unscaled by {improvement_dist:.2f} logometers\")\n",
    "    print(f\"  But still farther than original 'planet' ‚Üí 'Planet'\")\n",
    "else:\n",
    "    print(f\"‚ùå Scaled version is no improvement\")\n",
    "\n",
    "if cos_scaled_to_Planet > 0.71:\n",
    "    improvement_cos = cos_scaled_to_Planet - 0.71\n",
    "    print(f\"‚úÖ SCALED VERSION IS MORE ALIGNED by {improvement_cos:.4f}!\")\n",
    "elif cos_scaled_to_Planet > 0.51:\n",
    "    improvement_cos = cos_scaled_to_Planet - 0.51\n",
    "    print(f\"‚úì Scaled version is better than unscaled by {improvement_cos:.4f}\")\n",
    "    print(f\"  But still less aligned than original 'planet' ‚Üí 'Planet'\")\n",
    "else:\n",
    "    print(f\"‚ùå Scaled version is no improvement in alignment\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "azimuth",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
