{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find Clean Single-Token Words\n",
    "\n",
    "**Goal:** Find interesting words that tokenize to exactly 1 token (no BPE splitting) for neighborhood analysis.\n",
    "\n",
    "**Why:** We want to examine the local neighborhood around semantically meaningful tokens to test:\n",
    "1. **Radial hypothesis:** Do related words lie along the same ray at different radii?\n",
    "2. **Causal vs cosine:** What's in the causal hypersphere vs angular cone?\n",
    "\n",
    "**Method:**\n",
    "- Test various candidate words across semantic categories\n",
    "- Find which ones map to single tokens\n",
    "- Report token IDs and check if they're in our 32k sample\n",
    "\n",
    "**Next step:** Pick the most interesting token(s) and analyze their neighborhoods in 07.57."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration:\n",
      "  Model: Qwen/Qwen3-4B-Instruct-2507\n",
      "  Token sample: ../data/vectors/distances_causal_32000.pt\n"
     ]
    }
   ],
   "source": [
    "# Model\n",
    "MODEL_NAME = \"Qwen/Qwen3-4B-Instruct-2507\"\n",
    "\n",
    "# Our token sample (to check if candidates are available)\n",
    "TOKEN_INDICES_PATH = '../data/vectors/distances_causal_32000.pt'\n",
    "\n",
    "print(f\"Configuration:\")\n",
    "print(f\"  Model: {MODEL_NAME}\")\n",
    "print(f\"  Token sample: {TOKEN_INDICES_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Imports complete\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "\n",
    "print(\"✓ Imports complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer from Qwen/Qwen3-4B-Instruct-2507...\n",
      "\n",
      "✓ Tokenizer loaded\n",
      "  Vocab size: 151,643\n"
     ]
    }
   ],
   "source": [
    "print(f\"Loading tokenizer from {MODEL_NAME}...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "print(f\"\\n✓ Tokenizer loaded\")\n",
    "print(f\"  Vocab size: {tokenizer.vocab_size:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Token Sample Indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading token indices from ../data/vectors/distances_causal_32000.pt...\n",
      "\n",
      "✓ Loaded token sample\n",
      "  N tokens in sample: 32,000\n",
      "  Range: [5, 151930]\n",
      "  Converted to set for fast lookup\n"
     ]
    }
   ],
   "source": [
    "print(f\"Loading token indices from {TOKEN_INDICES_PATH}...\")\n",
    "data = torch.load(TOKEN_INDICES_PATH, weights_only=False)\n",
    "token_indices = data['token_indices'].numpy()\n",
    "\n",
    "print(f\"\\n✓ Loaded token sample\")\n",
    "print(f\"  N tokens in sample: {len(token_indices):,}\")\n",
    "print(f\"  Range: [{token_indices.min()}, {token_indices.max()}]\")\n",
    "\n",
    "# Convert to set for fast membership checking\n",
    "token_set = set(token_indices)\n",
    "print(f\"  Converted to set for fast lookup\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Candidate Words\n",
    "\n",
    "We'll test words across various semantic categories to find clean single-token words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing tokenization for candidate words...\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Animals:\n",
      "----------------------------------------\n",
      "  ✓ 'cat' → token 4616: 'cat' [✗ not in sample]\n",
      "  ✓ 'dog' → token 18457: 'dog' [✗ not in sample]\n",
      "  ✓ 'bird' → token 22592: 'bird' [✓ IN SAMPLE]\n",
      "  ✓ 'fish' → token 18170: 'fish' [✗ not in sample]\n",
      "  ✓ 'lion' → token 78151: 'lion' [✗ not in sample]\n",
      "  ✗ 'tiger' → 2 tokens: [83, 7272]\n",
      "  ✗ 'elephant' → 2 tokens: [10068, 26924]\n",
      "  ✗ 'whale' → 2 tokens: [1312, 1574]\n",
      "\n",
      "Royalty:\n",
      "----------------------------------------\n",
      "  ✓ 'king' → token 10566: 'king' [✗ not in sample]\n",
      "  ✓ 'queen' → token 93114: 'queen' [✗ not in sample]\n",
      "  ✗ 'monarch' → 2 tokens: [1645, 1113]\n",
      "  ✗ 'emperor' → 2 tokens: [336, 25819]\n",
      "  ✗ 'prince' → 2 tokens: [649, 1701]\n",
      "  ✗ 'princess' → 2 tokens: [649, 19570]\n",
      "  ✗ 'duke' → 2 tokens: [1054, 440]\n",
      "  ✓ 'lord' → token 25598: 'lord' [✗ not in sample]\n",
      "\n",
      "Intelligence:\n",
      "----------------------------------------\n",
      "  ✓ 'intelligence' → token 92275: 'intelligence' [✗ not in sample]\n",
      "  ✗ 'wisdom' → 2 tokens: [48043, 5600]\n",
      "  ✓ 'knowledge' → token 89053: 'knowledge' [✗ not in sample]\n",
      "  ✓ 'smart' → token 39803: 'smart' [✗ not in sample]\n",
      "  ✗ 'clever' → 2 tokens: [9148, 423]\n",
      "  ✗ 'genius' → 2 tokens: [4370, 9156]\n",
      "  ✗ 'brilliant' → 3 tokens: [1323, 483, 5372]\n",
      "\n",
      "Emotions:\n",
      "----------------------------------------\n",
      "  ✓ 'love' → token 30053: 'love' [✗ not in sample]\n",
      "  ✗ 'hate' → 2 tokens: [71, 349]\n",
      "  ✗ 'fear' → 2 tokens: [69, 682]\n",
      "  ✓ 'joy' → token 4123: 'joy' [✗ not in sample]\n",
      "  ✓ 'anger' → token 4003: 'anger' [✗ not in sample]\n",
      "  ✗ 'sadness' → 2 tokens: [82114, 2090]\n",
      "  ✗ 'happiness' → 2 tokens: [71, 66291]\n",
      "  ✗ 'surprise' → 2 tokens: [19784, 9671]\n",
      "\n",
      "Colors:\n",
      "----------------------------------------\n",
      "  ✓ 'red' → token 1151: 'red' [✗ not in sample]\n",
      "  ✓ 'blue' → token 12203: 'blue' [✗ not in sample]\n",
      "  ✓ 'green' → token 13250: 'green' [✗ not in sample]\n",
      "  ✓ 'yellow' → token 27869: 'yellow' [✗ not in sample]\n",
      "  ✓ 'purple' → token 56507: 'purple' [✗ not in sample]\n",
      "  ✓ 'orange' → token 34164: 'orange' [✗ not in sample]\n",
      "  ✓ 'black' → token 11453: 'black' [✓ IN SAMPLE]\n",
      "  ✓ 'white' → token 5782: 'white' [✓ IN SAMPLE]\n",
      "\n",
      "Numbers:\n",
      "----------------------------------------\n",
      "  ✓ 'one' → token 603: 'one' [✗ not in sample]\n",
      "  ✓ 'two' → token 19789: 'two' [✗ not in sample]\n",
      "  ✓ 'three' → token 27856: 'three' [✗ not in sample]\n",
      "  ✓ 'four' → token 34024: 'four' [✓ IN SAMPLE]\n",
      "  ✓ 'five' → token 52670: 'five' [✗ not in sample]\n",
      "  ✓ 'six' → token 50364: 'six' [✗ not in sample]\n",
      "  ✓ 'seven' → token 80185: 'seven' [✗ not in sample]\n",
      "  ✓ 'eight' → token 67532: 'eight' [✗ not in sample]\n",
      "  ✓ 'nine' → token 93223: 'nine' [✗ not in sample]\n",
      "  ✓ 'ten' → token 1960: 'ten' [✗ not in sample]\n",
      "\n",
      "Size:\n",
      "----------------------------------------\n",
      "  ✓ 'big' → token 16154: 'big' [✓ IN SAMPLE]\n",
      "  ✓ 'small' → token 9004: 'small' [✗ not in sample]\n",
      "  ✓ 'large' → token 16767: 'large' [✗ not in sample]\n",
      "  ✓ 'tiny' → token 46116: 'tiny' [✗ not in sample]\n",
      "  ✓ 'huge' → token 95370: 'huge' [✗ not in sample]\n",
      "  ✗ 'enormous' → 3 tokens: [268, 493, 782]\n",
      "  ✗ 'microscopic' → 2 tokens: [40443, 57410]\n",
      "\n",
      "Science:\n",
      "----------------------------------------\n",
      "  ✓ 'science' → token 39557: 'science' [✗ not in sample]\n",
      "  ✓ 'physics' → token 66765: 'physics' [✓ IN SAMPLE]\n",
      "  ✓ 'chemistry' → token 51655: 'chemistry' [✓ IN SAMPLE]\n",
      "  ✓ 'biology' → token 80062: 'biology' [✗ not in sample]\n",
      "  ✗ 'mathematics' → 2 tokens: [10374, 33705]\n",
      "  ✗ 'astronomy' → 2 tokens: [20467, 16974]\n",
      "\n",
      "Arts:\n",
      "----------------------------------------\n",
      "  ✓ 'art' → token 471: 'art' [✗ not in sample]\n",
      "  ✓ 'music' → token 31161: 'music' [✗ not in sample]\n",
      "  ✗ 'literature' → 2 tokens: [68091, 1568]\n",
      "  ✗ 'poetry' → 2 tokens: [5368, 15149]\n",
      "  ✗ 'painting' → 2 tokens: [33617, 287]\n",
      "  ✗ 'sculpture' → 3 tokens: [2388, 23396, 552]\n",
      "\n",
      "Abstract:\n",
      "----------------------------------------\n",
      "  ✗ 'beauty' → 2 tokens: [1371, 57951]\n",
      "  ✓ 'truth' → token 58577: 'truth' [✓ IN SAMPLE]\n",
      "  ✗ 'freedom' → 2 tokens: [24716, 8769]\n",
      "  ✓ 'justice' → token 38768: 'justice' [✗ not in sample]\n",
      "  ✓ 'peace' → token 54125: 'peace' [✗ not in sample]\n",
      "  ✓ 'war' → token 11455: 'war' [✓ IN SAMPLE]\n",
      "  ✓ 'time' → token 1678: 'time' [✓ IN SAMPLE]\n",
      "  ✓ 'space' → token 8746: 'space' [✓ IN SAMPLE]\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Candidate words organized by category\n",
    "test_words = {\n",
    "    'Animals': ['cat', 'dog', 'bird', 'fish', 'lion', 'tiger', 'elephant', 'whale'],\n",
    "    'Royalty': ['king', 'queen', 'monarch', 'emperor', 'prince', 'princess', 'duke', 'lord'],\n",
    "    'Intelligence': ['intelligence', 'wisdom', 'knowledge', 'smart', 'clever', 'genius', 'brilliant'],\n",
    "    'Emotions': ['love', 'hate', 'fear', 'joy', 'anger', 'sadness', 'happiness', 'surprise'],\n",
    "    'Colors': ['red', 'blue', 'green', 'yellow', 'purple', 'orange', 'black', 'white'],\n",
    "    'Numbers': ['one', 'two', 'three', 'four', 'five', 'six', 'seven', 'eight', 'nine', 'ten'],\n",
    "    'Size': ['big', 'small', 'large', 'tiny', 'huge', 'enormous', 'microscopic'],\n",
    "    'Science': ['science', 'physics', 'chemistry', 'biology', 'mathematics', 'astronomy'],\n",
    "    'Arts': ['art', 'music', 'literature', 'poetry', 'painting', 'sculpture'],\n",
    "    'Abstract': ['beauty', 'truth', 'freedom', 'justice', 'peace', 'war', 'time', 'space']\n",
    "}\n",
    "\n",
    "print(\"Testing tokenization for candidate words...\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "clean_tokens = []  # Will store (category, word, token_id, in_sample)\n",
    "\n",
    "for category, words in test_words.items():\n",
    "    print(f\"\\n{category}:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    for word in words:\n",
    "        tokens = tokenizer.encode(word, add_special_tokens=False)\n",
    "        \n",
    "        if len(tokens) == 1:\n",
    "            token_id = tokens[0]\n",
    "            token_text = tokenizer.decode([token_id])\n",
    "            in_sample = token_id in token_set\n",
    "            \n",
    "            status = \"✓ IN SAMPLE\" if in_sample else \"✗ not in sample\"\n",
    "            print(f\"  ✓ '{word}' → token {token_id}: '{token_text}' [{status}]\")\n",
    "            \n",
    "            clean_tokens.append((category, word, token_id, in_sample))\n",
    "        else:\n",
    "            print(f\"  ✗ '{word}' → {len(tokens)} tokens: {tokens}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of Clean Tokens in Sample\n",
    "\n",
    "These are the single-token words that are also in our 32k sample (available for neighborhood analysis)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Clean single-token words IN our 32k sample:\n",
      "================================================================================\n",
      "\n",
      "Animals:\n",
      "  • 'bird' (token 22592)\n",
      "\n",
      "Colors:\n",
      "  • 'black' (token 11453)\n",
      "  • 'white' (token 5782)\n",
      "\n",
      "Numbers:\n",
      "  • 'four' (token 34024)\n",
      "\n",
      "Size:\n",
      "  • 'big' (token 16154)\n",
      "\n",
      "Science:\n",
      "  • 'physics' (token 66765)\n",
      "  • 'chemistry' (token 51655)\n",
      "\n",
      "Abstract:\n",
      "  • 'truth' (token 58577)\n",
      "  • 'war' (token 11455)\n",
      "  • 'time' (token 1678)\n",
      "  • 'space' (token 8746)\n",
      "\n",
      "================================================================================\n",
      "Total: 11 clean tokens available for analysis\n"
     ]
    }
   ],
   "source": [
    "# Filter to only tokens in our sample\n",
    "available_tokens = [(cat, word, tid) for cat, word, tid, in_sample in clean_tokens if in_sample]\n",
    "\n",
    "print(f\"\\nClean single-token words IN our 32k sample:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if available_tokens:\n",
    "    # Group by category\n",
    "    by_category = {}\n",
    "    for cat, word, tid in available_tokens:\n",
    "        if cat not in by_category:\n",
    "            by_category[cat] = []\n",
    "        by_category[cat].append((word, tid))\n",
    "    \n",
    "    for category, tokens in by_category.items():\n",
    "        print(f\"\\n{category}:\")\n",
    "        for word, tid in tokens:\n",
    "            print(f\"  • '{word}' (token {tid})\")\n",
    "    \n",
    "    print(f\"\\n\" + \"=\" * 80)\n",
    "    print(f\"Total: {len(available_tokens)} clean tokens available for analysis\")\n",
    "    \n",
    "else:\n",
    "    print(\"\\n⚠️  No clean single-token words found in our sample!\")\n",
    "    print(\"    We may need to expand our search or use a different approach.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommendations for Neighborhood Analysis\n",
    "\n",
    "Based on the results above, which tokens would be most interesting to examine?\n",
    "\n",
    "**Criteria:**\n",
    "1. **Semantic richness:** Words with clear semantic relationships (e.g., 'king' → 'queen', 'monarch')\n",
    "2. **Conceptual clarity:** Abstract concepts vs concrete objects\n",
    "3. **Category diversity:** Test different semantic domains\n",
    "\n",
    "**Linear representation hypothesis test:**\n",
    "- If 'king', 'queen', 'monarch' are along the same ray → supports hypothesis\n",
    "- If 'cat', 'dog', 'lion' cluster in causal space → interesting structure\n",
    "- If abstract concepts ('love', 'wisdom') show radial stratification → deep finding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Additional candidates:\n",
      "================================================================================\n",
      "  ✓ 'the' → token 1782: 'the' [✓ IN SAMPLE]\n",
      "  ✓ 'and' → token 437: 'and' [✗ not in sample]\n",
      "  ✓ 'is' → token 285: 'is' [✗ not in sample]\n",
      "  ✓ 'to' → token 983: 'to' [✓ IN SAMPLE]\n",
      "  ✓ 'of' → token 1055: 'of' [✗ not in sample]\n",
      "  ✓ 'AI' → token 15469: 'AI' [✗ not in sample]\n",
      "  ✓ 'DNA' → token 55320: 'DNA' [✗ not in sample]\n",
      "  ✓ 'USA' → token 24347: 'USA' [✗ not in sample]\n",
      "  ✓ 'CEO' → token 78496: 'CEO' [✗ not in sample]\n",
      "  ✓ 'hello' → token 14990: 'hello' [✗ not in sample]\n",
      "  ✗ 'goodbye' → 2 tokens\n",
      "  ✓ 'yes' → token 9693: 'yes' [✗ not in sample]\n",
      "  ✓ 'no' → token 2152: 'no' [✗ not in sample]\n",
      "  ✓ 'water' → token 12987: 'water' [✗ not in sample]\n",
      "  ✓ 'fire' → token 10796: 'fire' [✓ IN SAMPLE]\n",
      "  ✓ 'earth' → token 27541: 'earth' [✗ not in sample]\n",
      "  ✓ 'air' → token 1310: 'air' [✗ not in sample]\n",
      "  ✓ 'sun' → token 39519: 'sun' [✗ not in sample]\n",
      "  ✓ 'moon' → token 67269: 'moon' [✗ not in sample]\n",
      "  ✓ 'star' → token 11870: 'star' [✗ not in sample]\n",
      "  ✓ 'planet' → token 50074: 'planet' [✓ IN SAMPLE]\n",
      "  ✓ 'good' → token 18536: 'good' [✗ not in sample]\n",
      "  ✓ 'bad' → token 13855: 'bad' [✗ not in sample]\n",
      "  ✓ 'evil' → token 58867: 'evil' [✗ not in sample]\n",
      "  ✓ 'pure' → token 51733: 'pure' [✗ not in sample]\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Total available tokens: 15\n"
     ]
    }
   ],
   "source": [
    "# Let's also check some interesting multi-word concepts that might be single tokens\n",
    "additional_candidates = [\n",
    "    'the', 'and', 'is', 'to', 'of',  # Common words (likely single tokens)\n",
    "    'AI', 'DNA', 'USA', 'CEO',  # Acronyms\n",
    "    'hello', 'goodbye', 'yes', 'no',  # Simple words\n",
    "    'water', 'fire', 'earth', 'air',  # Elements\n",
    "    'sun', 'moon', 'star', 'planet',  # Astronomy\n",
    "    'good', 'bad', 'evil', 'pure',  # Morality\n",
    "]\n",
    "\n",
    "print(\"\\nAdditional candidates:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for word in additional_candidates:\n",
    "    tokens = tokenizer.encode(word, add_special_tokens=False)\n",
    "    \n",
    "    if len(tokens) == 1:\n",
    "        token_id = tokens[0]\n",
    "        token_text = tokenizer.decode([token_id])\n",
    "        in_sample = token_id in token_set\n",
    "        \n",
    "        status = \"✓ IN SAMPLE\" if in_sample else \"✗ not in sample\"\n",
    "        print(f\"  ✓ '{word}' → token {token_id}: '{token_text}' [{status}]\")\n",
    "        \n",
    "        if in_sample:\n",
    "            available_tokens.append((\"Additional\", word, token_id))\n",
    "    else:\n",
    "        print(f\"  ✗ '{word}' → {len(tokens)} tokens\")\n",
    "\n",
    "print(f\"\\n{'=' * 80}\")\n",
    "print(f\"\\nTotal available tokens: {len(available_tokens)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "**For 07.57:** Pick 2-3 interesting tokens from the available list and:\n",
    "\n",
    "1. **Causal neighborhood:** Find all tokens within radius R (e.g., 5 logometers)\n",
    "2. **Cosine neighborhood:** Find tokens with cosine similarity > 0.999\n",
    "3. **Compare:** Do they overlap? Are cosine-similar tokens at different radii?\n",
    "4. **Decode:** What do the neighbors actually say?\n",
    "\n",
    "**Test the linear representation hypothesis:**\n",
    "- Are semantically related words along the same ray?\n",
    "- Does radial position correlate with frequency, specificity, or abstraction?\n",
    "- Do different semantic categories show different geometric patterns?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "azimuth",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
