{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Densest Point Neighborhood Analysis\n",
    "\n",
    "**Goal:** Explore the token at the highest density point and its neighborhood.\n",
    "\n",
    "**Method:**\n",
    "1. Find token with maximum k-NN density\n",
    "2. Get k nearest neighbors in **causal metric** (using precomputed distances)\n",
    "3. Get k nearest neighbors in **cosine similarity** (raw embeddings)\n",
    "4. Compare: Do causal metric neighborhoods = semantic neighborhoods?\n",
    "5. Decode tokens to see what they actually are\n",
    "\n",
    "**Key question:** Does the causal metric capture semantic structure, or something else?\n",
    "\n",
    "**Expected runtime:** ~2 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration:\n",
      "  Model: Qwen/Qwen3-4B-Instruct-2507\n",
      "  k-NN: 20 neighbors\n",
      "  Show top: 30 neighbors\n"
     ]
    }
   ],
   "source": [
    "# Input files\n",
    "DISTANCES_PATH = '../data/vectors/distances_causal_32000_full.npy'\n",
    "METADATA_PATH = '../data/vectors/distances_causal_32000.pt'\n",
    "METRIC_TENSOR_PATH = '../data/vectors/causal_metric_tensor_qwen3_4b.pt'\n",
    "\n",
    "# Model\n",
    "MODEL_NAME = 'Qwen/Qwen3-4B-Instruct-2507'\n",
    "\n",
    "# Analysis parameters\n",
    "K_NEIGHBORS = 20  # Number of neighbors to examine\n",
    "TOP_N = 30        # Show top N neighbors\n",
    "\n",
    "print(f\"Configuration:\")\n",
    "print(f\"  Model: {MODEL_NAME}\")\n",
    "print(f\"  k-NN: {K_NEIGHBORS} neighbors\")\n",
    "print(f\"  Show top: {TOP_N} neighbors\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Imports complete\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import pandas as pd\n",
    "\n",
    "print(\"✓ Imports complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading distance matrix...\n",
      "Loading metadata...\n",
      "\n",
      "✓ Loaded data\n",
      "  32,000 tokens in sample\n",
      "  Token indices: [5, 151930]\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading distance matrix...\")\n",
    "distances = np.load(DISTANCES_PATH)\n",
    "N = distances.shape[0]\n",
    "\n",
    "print(\"Loading metadata...\")\n",
    "metadata = torch.load(METADATA_PATH, weights_only=False)\n",
    "token_indices = metadata['token_indices'].numpy()\n",
    "\n",
    "print(f\"\\n✓ Loaded data\")\n",
    "print(f\"  {N:,} tokens in sample\")\n",
    "print(f\"  Token indices: [{token_indices.min()}, {token_indices.max()}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute k-NN Densities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Computing k-NN densities...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/k5/44vd1ct56xj4y9h7x213kvjr0000gn/T/ipykernel_27030/171613569.py:11: RuntimeWarning: Mean of empty slice.\n",
      "  mean_dist = k_nearest.mean()\n",
      "/Users/jefferyharrell/Projects/Azimuth/.venv/lib/python3.12/site-packages/numpy/_core/_methods.py:144: RuntimeWarning: invalid value encountered in divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Computed densities\n",
      "\n",
      "Density statistics:\n",
      "  Min: 0.000000\n",
      "  Max: 2413.602051\n",
      "  Mean: 0.595077\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nComputing k-NN densities...\")\n",
    "\n",
    "densities = np.zeros(N)\n",
    "mean_distances = np.zeros(N)\n",
    "\n",
    "for i in range(N):\n",
    "    dists_i = distances[i]\n",
    "    k_nearest = np.partition(dists_i, K_NEIGHBORS)[:K_NEIGHBORS+1]\n",
    "    k_nearest = k_nearest[k_nearest > 0][:K_NEIGHBORS]\n",
    "    \n",
    "    mean_dist = k_nearest.mean()\n",
    "    mean_distances[i] = mean_dist\n",
    "    densities[i] = 1.0 / mean_dist if mean_dist > 0 else 0\n",
    "\n",
    "print(f\"✓ Computed densities\")\n",
    "print(f\"\\nDensity statistics:\")\n",
    "print(f\"  Min: {densities.min():.6f}\")\n",
    "print(f\"  Max: {densities.max():.6f}\")\n",
    "print(f\"  Mean: {densities.mean():.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find Densest Point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "DENSEST POINT ANALYSIS\n",
      "======================================================================\n",
      "\n",
      "Densest point:\n",
      "  Sample index: 734\n",
      "  Vocabulary token ID: 141503\n",
      "  Density: 2413.602051 (1/logometers)\n",
      "  Mean distance to 20 neighbors: 0.0004 logometers\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"DENSEST POINT ANALYSIS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Find densest token in our sample\n",
    "densest_idx = np.argmax(densities)\n",
    "densest_token_id = token_indices[densest_idx]\n",
    "densest_density = densities[densest_idx]\n",
    "densest_mean_dist = mean_distances[densest_idx]\n",
    "\n",
    "print(f\"\\nDensest point:\")\n",
    "print(f\"  Sample index: {densest_idx}\")\n",
    "print(f\"  Vocabulary token ID: {densest_token_id}\")\n",
    "print(f\"  Density: {densest_density:.6f} (1/logometers)\")\n",
    "print(f\"  Mean distance to {K_NEIGHBORS} neighbors: {densest_mean_dist:.4f} logometers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading model and tokenizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa04837cbb5e40f99804d10cc86dd674",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Loaded model\n",
      "  Vocabulary size: 151,936\n",
      "  Embedding dimension: 2,560\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nLoading model and tokenizer...\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.float32,\n",
    "    device_map='cpu',\n",
    ")\n",
    "\n",
    "gamma = model.lm_head.weight.data\n",
    "del model\n",
    "\n",
    "print(f\"✓ Loaded model\")\n",
    "print(f\"  Vocabulary size: {gamma.shape[0]:,}\")\n",
    "print(f\"  Embedding dimension: {gamma.shape[1]:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Sampled Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting embeddings for sampled tokens...\n",
      "✓ Extracted embeddings\n",
      "  Shape: torch.Size([32000, 2560])\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nExtracting embeddings for sampled tokens...\")\n",
    "\n",
    "sampled_embeddings = gamma[token_indices]  # [N, hidden_dim]\n",
    "\n",
    "print(f\"✓ Extracted embeddings\")\n",
    "print(f\"  Shape: {sampled_embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find k-NN in Causal Metric\n",
    "\n",
    "These are already computed! Just sort the distance matrix row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Finding k nearest neighbors in CAUSAL METRIC...\n",
      "✓ Found 30 nearest neighbors in causal metric\n",
      "  Distance range: [0.0000, 0.0004] logometers\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nFinding k nearest neighbors in CAUSAL METRIC...\")\n",
    "\n",
    "# Get distances from densest point to all others\n",
    "causal_dists = distances[densest_idx]\n",
    "\n",
    "# Sort to get nearest neighbors (excluding self at distance 0)\n",
    "causal_sorted = np.argsort(causal_dists)\n",
    "causal_knn_indices = causal_sorted[1:TOP_N+1]  # Skip self (index 0)\n",
    "causal_knn_dists = causal_dists[causal_knn_indices]\n",
    "causal_knn_token_ids = token_indices[causal_knn_indices]\n",
    "\n",
    "print(f\"✓ Found {TOP_N} nearest neighbors in causal metric\")\n",
    "print(f\"  Distance range: [{causal_knn_dists.min():.4f}, {causal_knn_dists.max():.4f}] logometers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find k-NN in Cosine Similarity\n",
    "\n",
    "Compute cosine similarity to all tokens in sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Finding k nearest neighbors in COSINE SIMILARITY...\n",
      "✓ Found 30 nearest neighbors in cosine similarity\n",
      "  Similarity range: [1.0000, 1.0000]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nFinding k nearest neighbors in COSINE SIMILARITY...\")\n",
    "\n",
    "# Get embedding of densest point\n",
    "densest_embedding = sampled_embeddings[densest_idx]  # [hidden_dim]\n",
    "\n",
    "# Compute cosine similarities to all sampled tokens\n",
    "# cosine_sim = (u · v) / (||u|| ||v||)\n",
    "norms = torch.norm(sampled_embeddings, dim=1)  # [N]\n",
    "densest_norm = torch.norm(densest_embedding)\n",
    "\n",
    "dot_products = sampled_embeddings @ densest_embedding  # [N]\n",
    "cosine_sims = dot_products / (norms * densest_norm)\n",
    "cosine_sims = cosine_sims.numpy()\n",
    "\n",
    "# Sort by similarity (highest first)\n",
    "cosine_sorted = np.argsort(cosine_sims)[::-1]\n",
    "cosine_knn_indices = cosine_sorted[1:TOP_N+1]  # Skip self (similarity = 1.0)\n",
    "cosine_knn_sims = cosine_sims[cosine_knn_indices]\n",
    "cosine_knn_token_ids = token_indices[cosine_knn_indices]\n",
    "\n",
    "print(f\"✓ Found {TOP_N} nearest neighbors in cosine similarity\")\n",
    "print(f\"  Similarity range: [{cosine_knn_sims.min():.4f}, {cosine_knn_sims.max():.4f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decode Tokens\n",
    "\n",
    "Convert token IDs to actual text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Decoded tokens\n"
     ]
    }
   ],
   "source": [
    "def decode_token(token_id):\n",
    "    \"\"\"Decode a single token ID to text.\"\"\"\n",
    "    try:\n",
    "        text = tokenizer.decode([token_id])\n",
    "        # Clean up: show repr for special chars\n",
    "        if text.strip() == '':\n",
    "            return repr(text)\n",
    "        return text\n",
    "    except:\n",
    "        return \"<ERROR>\"\n",
    "\n",
    "# Decode densest token\n",
    "densest_text = decode_token(densest_token_id)\n",
    "\n",
    "# Decode causal neighbors\n",
    "causal_texts = [decode_token(tid) for tid in causal_knn_token_ids]\n",
    "\n",
    "# Decode cosine neighbors\n",
    "cosine_texts = [decode_token(tid) for tid in cosine_knn_token_ids]\n",
    "\n",
    "print(f\"\\n✓ Decoded tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display Results: Densest Token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "DENSEST TOKEN\n",
      "======================================================================\n",
      "\n",
      "Token ID: 141503\n",
      "Text: การบริหาร\n",
      "Density: 2413.602051 (1/logometers)\n",
      "Mean neighbor distance: 0.0004 logometers\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"DENSEST TOKEN\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\\nToken ID: {densest_token_id}\")\n",
    "print(f\"Text: {densest_text}\")\n",
    "print(f\"Density: {densest_density:.6f} (1/logometers)\")\n",
    "print(f\"Mean neighbor distance: {densest_mean_dist:.4f} logometers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display Results: Causal Metric Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "TOP 30 NEIGHBORS: CAUSAL METRIC (Distance-based)\n",
      "======================================================================\n",
      "\n",
      " Rank  Token ID  Distance          Text\n",
      "    1       180  0.000000             �\n",
      "    2    138979  0.000000        รับรอง\n",
      "    3    124707  0.000345            ต์\n",
      "    4    134995  0.000345        เสี่ยง\n",
      "    5    123848  0.000423             �\n",
      "    6    127321  0.000423          นั่น\n",
      "    7    131745  0.000423          ล้าน\n",
      "    8    129342  0.000423           ใส่\n",
      "    9    126637  0.000423        เกี่ยว\n",
      "   10    126668  0.000423          ยิ่ง\n",
      "   11    132276  0.000423         พิมพ์\n",
      "   12    147628  0.000423             類\n",
      "   13    142915  0.000423          ตุลา\n",
      "   14    132108  0.000423       ป้องกัน\n",
      "   15    132098  0.000423     เทคโนโลยี\n",
      "   16    149987  0.000423             梁\n",
      "   17    132715  0.000423          พนัน\n",
      "   18    136380  0.000423        ทุกวัน\n",
      "   19    130159  0.000423            فَ\n",
      "   20    149257  0.000423             輪\n",
      "   21    126324  0.000423          เป็น\n",
      "   22    130711  0.000423         ศูนย์\n",
      "   23    143257  0.000423 คาสิโนออนไลน์\n",
      "   24    139472  0.000423            สื\n",
      "   25    149255  0.000423             露\n",
      "   26    148359  0.000423             列\n",
      "   27    124831  0.000423          ส่วน\n",
      "   28    132581  0.000423      เดียวกัน\n",
      "   29    125062  0.000423           ถูก\n",
      "   30    139397  0.000423         คล้าย\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(f\"TOP {TOP_N} NEIGHBORS: CAUSAL METRIC (Distance-based)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "causal_df = pd.DataFrame({\n",
    "    'Rank': range(1, TOP_N + 1),\n",
    "    'Token ID': causal_knn_token_ids,\n",
    "    'Distance': causal_knn_dists,\n",
    "    'Text': causal_texts\n",
    "})\n",
    "\n",
    "print(\"\\n\" + causal_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display Results: Cosine Similarity Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "TOP 30 NEIGHBORS: COSINE SIMILARITY (Embedding-based)\n",
      "======================================================================\n",
      "\n",
      " Rank  Token ID  Similarity           Text\n",
      "    1       180    1.000002              �\n",
      "    2    138979    1.000002         รับรอง\n",
      "    3    124707    1.000002             ต์\n",
      "    4    141503    1.000002      การบริหาร\n",
      "    5    132709    1.000002          ว่าจะ\n",
      "    6    138035    1.000002            รีบ\n",
      "    7    151869    1.000002             ''\n",
      "    8    131078    1.000002         เข้าไป\n",
      "    9    151814    1.000002             ''\n",
      "   10    135773    1.000002       การพัฒนา\n",
      "   11    124212    1.000002           ื่อง\n",
      "   12    124484    1.000002             นั\n",
      "   13    151654    1.000002 <|vision_pad|>\n",
      "   14    137681    1.000002          ชัดเจ\n",
      "   15    141155    1.000002             ผู\n",
      "   16    124383    1.000002           ต้อง\n",
      "   17    140928    1.000002            บัง\n",
      "   18    132742    1.000002    เจ้าหน้าที่\n",
      "   19    128190    1.000002             ศิ\n",
      "   20    136561    1.000002          อาศัย\n",
      "   21    126770    1.000002            พัฒ\n",
      "   22    126432    1.000002          ชีวิต\n",
      "   23    129049    1.000002          บริษั\n",
      "   24    132581    1.000002       เดียวกัน\n",
      "   25    134614    1.000002           อดีต\n",
      "   26    129978    1.000002          เขียน\n",
      "   27    139761    1.000002         ปริมาณ\n",
      "   28    135582    1.000002          ชุมชน\n",
      "   29    141495    1.000002         พฤศจิก\n",
      "   30    147836    1.000002              אַ\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(f\"TOP {TOP_N} NEIGHBORS: COSINE SIMILARITY (Embedding-based)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "cosine_df = pd.DataFrame({\n",
    "    'Rank': range(1, TOP_N + 1),\n",
    "    'Token ID': cosine_knn_token_ids,\n",
    "    'Similarity': cosine_knn_sims,\n",
    "    'Text': cosine_texts\n",
    "})\n",
    "\n",
    "print(\"\\n\" + cosine_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison: Overlap Between Neighborhoods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "NEIGHBORHOOD OVERLAP ANALYSIS\n",
      "======================================================================\n",
      "\n",
      "Tokens in both neighborhoods: 4 / 30 (13.3%)\n",
      "\n",
      "Overlapping tokens:\n",
      "  124707: ต์                   (causal rank:  3, cosine rank:  3)\n",
      "     180: �                    (causal rank:  1, cosine rank:  1)\n",
      "  132581: เดียวกัน             (causal rank: 28, cosine rank: 24)\n",
      "  138979: รับรอง               (causal rank:  2, cosine rank:  2)\n",
      "\n",
      "======================================================================\n",
      "INTERPRETATION:\n",
      "======================================================================\n",
      "\n",
      "❌ LOW OVERLAP (13.3%)\n",
      "   Causal metric neighborhoods ≠ semantic neighborhoods\n",
      "   The causal metric captures different structure than raw embeddings!\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"NEIGHBORHOOD OVERLAP ANALYSIS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Find tokens that appear in both neighborhoods\n",
    "causal_set = set(causal_knn_token_ids)\n",
    "cosine_set = set(cosine_knn_token_ids)\n",
    "overlap = causal_set & cosine_set\n",
    "\n",
    "overlap_pct = 100 * len(overlap) / TOP_N\n",
    "\n",
    "print(f\"\\nTokens in both neighborhoods: {len(overlap)} / {TOP_N} ({overlap_pct:.1f}%)\")\n",
    "\n",
    "if len(overlap) > 0:\n",
    "    print(f\"\\nOverlapping tokens:\")\n",
    "    for token_id in overlap:\n",
    "        text = decode_token(token_id)\n",
    "        causal_rank = np.where(causal_knn_token_ids == token_id)[0][0] + 1\n",
    "        cosine_rank = np.where(cosine_knn_token_ids == token_id)[0][0] + 1\n",
    "        print(f\"  {token_id:6d}: {text:20s} (causal rank: {causal_rank:2d}, cosine rank: {cosine_rank:2d})\")\n",
    "\n",
    "print(f\"\\n\" + \"=\" * 70)\n",
    "print(\"INTERPRETATION:\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "if overlap_pct > 70:\n",
    "    print(f\"\\n✅ HIGH OVERLAP ({overlap_pct:.1f}%)\")\n",
    "    print(\"   Causal metric neighborhoods ≈ semantic neighborhoods\")\n",
    "    print(\"   The causal metric captures semantic structure!\")\n",
    "elif overlap_pct > 30:\n",
    "    print(f\"\\n⚠️  MODERATE OVERLAP ({overlap_pct:.1f}%)\")\n",
    "    print(\"   Some agreement between causal and cosine metrics\")\n",
    "    print(\"   Causal metric partially captures semantic structure\")\n",
    "else:\n",
    "    print(f\"\\n❌ LOW OVERLAP ({overlap_pct:.1f}%)\")\n",
    "    print(\"   Causal metric neighborhoods ≠ semantic neighborhoods\")\n",
    "    print(\"   The causal metric captures different structure than raw embeddings!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**Question:** Does the causal metric capture semantic neighborhoods?\n",
    "\n",
    "**Method:** Compare k-NN in causal metric vs cosine similarity for densest point\n",
    "\n",
    "**Answer:** Check the overlap percentage and decoded tokens above!\n",
    "\n",
    "---\n",
    "\n",
    "**Key insights:**\n",
    "- Densest point likely in r~22 cluster (from 07.54)\n",
    "- Causal neighbors: found via distance in M-metric\n",
    "- Cosine neighbors: found via raw embedding similarity\n",
    "- Overlap reveals if causal metric = semantic structure"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "azimuth",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
